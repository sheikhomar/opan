{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 24: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.summary {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".insight {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".sidenote {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".warning {\n",
       "  border: solid 1px #8a6d3b !important;\n",
       "  background: #fcf8e3 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".green {\n",
       "  color: #006600 !important;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  color: #cc0000 !important;\n",
       "}\n",
       "\n",
       ".simplex-tableaux {\n",
       "    font-size: 13pt !important;\n",
       "}\n",
       "\n",
       ".last-row td {\n",
       "    border-top: solid 1px Black !important;\n",
       "}\n",
       "\n",
       ".smallest-value {\n",
       "    background-color: Green !important;\n",
       "    color: White !important;\n",
       "    font-weight: bold !important;\n",
       "}</style>Stylesheet \"styles.css\" loaded."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "from fractions import Fraction\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "utils.load_custom_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.1\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.1.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the proof, we ignore the bias terms, use the identify function as the activation function for all neurons and have only a single neuron in the output layer.\n",
    "\n",
    "Suppose we have a two-layer network with a single output. The output of this network is:\n",
    "$$\n",
    "o_{2L} = \\mathbf{w}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Now, suppose we have a three-layer network. The output of this network can be computed in steps.\n",
    "\n",
    "$$\n",
    "o_{3L} = \\mathbf{w}^{(2)T} \\mathbf{h}\n",
    "$$\n",
    "\n",
    "The $\\mathbf{h}$ is computed as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \\mathbf{W}^{(1)T} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Combining the two expressions, we get:\n",
    "\n",
    "$$\n",
    "o_{3L} = \\mathbf{w}^{(2)T} \\mathbf{W}^{(1)T} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "The expression above can be simplied:\n",
    "$$\n",
    "o_{3L} = \\mathbf{w}^T_{c} \\mathbf{x}\n",
    "$$\n",
    "where $\\mathbf{w}_{c} =\\mathbf{w}^{(2)T} \\mathbf{W}^{(1)T}$\n",
    "\n",
    "This shows that the three-layer network and two layer network are equivalent when the activation function for all neurons are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-layer network using only linear activation functions cannot define a nonlinear decision function because the output of such a network expresses a linear combination of $\\mathbf{x}$ i.e., the network can only learn a linear decision function. Since we showed that a three-layer network with linear activation function is equivalent to a two-layer network, this 3-layer network cannot learn a nonlinear decision function despite its extra layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.2\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.2.0.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 6.2.1\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.2.1.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network has two weight matrices:\n",
    "- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{D \\times L}$, between input layer and the hidden layer\n",
    "- $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{L \\times K}$, between hidden layer and the output layer\n",
    "\n",
    "The number of weights is: $D\\cdot L + L\\cdot K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 6.2.2\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.2.2.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer has $L$ neurons. Each neuron in the hidden layer requires $D$ multiplications and $D-1$ additions.\n",
    "\n",
    "The output layer has $K$ neurons. Each neuron in the output layer requires $L$ multiplications and $L-1$ additions\n",
    "\n",
    "\n",
    "Therefore, we need:\n",
    "- $L \\cdot  D    + K \\cdot L$ multiplications\n",
    "- $L \\cdot (D-1) + K \\cdot (L-1)$ additions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.3\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.3.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Equation (6.2) in question is given:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/homework-24/eq-6.2.png\" width=\"800\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the entries of the weight vector are all the same value $C$ i.e., $\\mathbf{W}_k^{(1)}=[C, C, \\cdots, C]^T$, then we have:\n",
    "\n",
    "$$\n",
    "h_k =  f \\left(  \\sum_{i=1}^{D} C x_i  \\right) =  f \\left( C \\sum_{i=1}^{D}  x_i  \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $h_k$ is constant for $k = 1, \\cdots, K$ i.e., all neurons in the hidden layer will produce the same value. This is equivalent to having a hidden layer with a single neuron which acts like a bottleneck in the network. The output of the hidden layer will be a vector whose entries are all the same. Essentially, this hidden layer will reduce the input $\\mathbf{x} \\in \\mathbb{R}^{D}$ to $h_k \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.4\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.4.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/homework-24/3-layer-nn-training-algo.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.5\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.5.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/homework-24/rbf-training.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 6.6\n",
    "\n",
    "\n",
    "<img src=\"figures/homework-24/ex6.6.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
