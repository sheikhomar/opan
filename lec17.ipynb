{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 17: Bayesian Decision Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.summary {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".insight {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".sidenote {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".warning {\n",
       "  border: solid 1px #8a6d3b !important;\n",
       "  background: #fcf8e3 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".green {\n",
       "  color: #006600 !important;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  color: #cc0000 !important;\n",
       "}</style>Stylesheet \"styles.css\" loaded."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "utils.load_custom_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification. This approach is based on quantifying the tradeoffs between various classification decisions using probability and the costs that accompany\n",
    "such decisions. It makes the assumption that the decision problem is posed in probabilistic terms, and that all of the relevant probability values are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Definitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/dataset-example.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Functions\n",
    "- Probability mass function, denoted $P(\\cdot)$, used for discrete variables\n",
    "- Probability density function, denoted $p(\\cdot)$, used for continuous variables. Density functions are normalized, so the area under each curve is 1.0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### A priori probability\n",
    "A priori probability (prior probability) captures an expert's beliefs about something before looking at new data. Suppose we have a dataset of 10 samples categorised into two classes $c_1$ and $c_2$. The a priori probability can be computed as follows:\n",
    "\n",
    "$$\n",
    "P(c_k) = \\frac{\\text{number of samples in } c_k}{\\text{total number of samples}} \\text{ or }\\\\\n",
    "p(x) = \\frac{\\text{number of } x}{\\text{total number of samples}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our toy dataset, we can compute the a priori probabilities for $c_k$ as follows:\n",
    "\n",
    "$$\n",
    "P(c_1) = \\frac{5}{10}, \n",
    "P(c_2) = \\frac{5}{10}, \n",
    "$$\n",
    "\n",
    "and for $x$:\n",
    "\\begin{align}\n",
    "p(x=29) = \\frac{1}{10} \\\\\n",
    "p(x=30) = \\frac{1}{10} \\\\\n",
    "p(x=31) = \\frac{3}{10} \\\\\n",
    "p(x=49) = \\frac{1}{10} \\\\\n",
    "p(x=50) = \\frac{1}{10} \\\\\n",
    "p(x=51) = \\frac{3}{10}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The a priori values must sum to 1 because they are probability values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making decision based solely on the a priori probability is like deciding whether a picture is $c_1$ or $c_2$ without actually seeing the picture first. If we are forced to make such a decision with so little information, we could use the following decision rule:\n",
    "\n",
    "**Classify $c_1$ if $P(c_1) > P(c_2)$; otherwise classify as $c_2$**\n",
    "\n",
    "This decision rule based solely on the a priori probabilites does not make sense to use for all images because a priori probabilities do not take into account the sample itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conditional Probability\n",
    "\n",
    "We need to define a way to obtain a probability given the sample. The conditional probability, denoted $P(c_k \\mid x)$, expresses the probability of class $c_k$ given a sample $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Class-conditional Probability\n",
    "\n",
    "We denote $p(x \\mid c_k)$ as the **class-conditional probability**. This expresses the probability of\n",
    "observing $x$ given that the sample it corresponds to belongs to class $c_k$.\n",
    "\n",
    "The class-conditional probability is also called the **likelihood** of $c_k$ with respect to $x$. This term is \n",
    "chosen to indicate that, other things being equal, the category $c_k$ for which $p(x \\mid c_k)$\n",
    "is large is more \"likely\" to be the true category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two hypothetical class-conditional probability density functions show the\n",
    "probability density of measuring a particular feature value $x$ given the pattern is\n",
    "in class $c_k$:\n",
    "\n",
    "<img src=\"figures/lecture-17/probability-density-functions.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Joint Probability\n",
    "\n",
    "We denote $p(c_k, x)$ as the **joint probability** of $c_k$ and $\\mathbf{x}$ and define it as:\n",
    "\n",
    "$$\n",
    "p(c_k, x) = P(c_k \\mid x) p(x) = p(x\\mid c_k) P(c_k)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^K{ p(x \\mid c_k) P(c_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bayes' formula\n",
    "\n",
    "Bayes' formula gives us a way to compute the conditional probability by rewriting the joint probability:\n",
    "\n",
    "\\begin{align}\n",
    "p(c_k, x) = P(c_k \\mid x) p(x) \\Longleftrightarrow\n",
    "\\frac{p(c_k, x)}{p(x)} = P(c_k \\mid x)\n",
    "\\end{align}\n",
    "\n",
    "and since $p(c_k, x) = p(x\\mid c_k) P(c_k)$, we can obtain Bayes' formula:\n",
    "\n",
    "\\begin{align}\n",
    "P(c_k \\mid x) = \\frac{p(x\\mid c_k) P(c_k)}{p(x)}\n",
    "\\end{align}\n",
    "\n",
    "Bayes' formula can be expressed informally in English by saying that:\n",
    "\n",
    "$$\n",
    "posterior = \\frac{likelihood \\times prior}{evidence}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the product of the likelihood and the prior probability that is most important in determining the posterior\n",
    "probability. The evidence factor, $p(x)$, can be viewed as merely a scale factor that guarantees that the posterior probabilities sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Examples: Using Bayes' formula\n",
    "\n",
    "$$\n",
    "P(c_1 \\mid x=30) = \\frac{ p(x=30 \\mid c_1) P(c_1) }{ p(x=30) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bayes' Decision Rule\n",
    "\n",
    "Suppose we can compute the conditional probabilites, how do we classify an observation $x$? We can use the following rule called Bayes' Decision Rule:\n",
    "\n",
    "**Classify $x$ as $c_1$ if $P(c_1 \\mid x) > P(c_2 \\mid x)$; otherwise classify $x$ as $c_2$**\n",
    "\n",
    "This form of the decision rule emphasizes the role of the posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above decision rule can be extended to more than two classes in a straightforward manner. Given:\n",
    "- a set of classes $\\{ c_1, c_2, \\cdots, c_K \\}$, \n",
    "- an observation $x$ and\n",
    "- the corresponding conditional probabilities $P(c_k \\mid x)$ where $k = 1, 2, \\cdots , K$ \n",
    "the decision rule is:\n",
    "\n",
    "**Classify $x$ as $c_l$ for which $P(c_l \\mid x) > P(c_i \\mid x)$ for all $i \\not = l$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Probability of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To justify Bayes' Decision Rule, let us calculate the probability of error whenever we make a decision.\n",
    "Whenever we classify a particular $x$, we can define the probability of error as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/error-probability.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (3.7) can be written as:\n",
    "$$\n",
    "P(error \\mid x) = \\min \\left[ P \\left( c_1 \\mid x \\right), P \\left( c_2 \\mid x \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, for a given $x$ we can minimize the probability of error by deciding $c_1$ if $P(c_1 \\mid x) > P(c_2 \\mid x)$ and $c_2$ otherwise. Will this rule minimize the average probability of error? Yes, because the average probability of error is given by\n",
    "\n",
    "<img src=\"figures/lecture-17/average-error.png\" width=\"600\" />\n",
    "\n",
    "and if for every $x$ we ensure that $P(error \\mid x)$ is as small as possible, then the integral must be as small as possible. \n",
    "\n",
    "Thus we have justified the following Bayes' decision rule for minimizing the probability of error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Decision Rule is given as:\n",
    "\n",
    "**Classify $x$ as $c_1$ if $P(c_1 \\mid x) > P(c_2 \\mid x)$; otherwise classify $x$ as $c_2$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the optimal decision function corresponds to the $x$ value where:\n",
    "\n",
    "$$\n",
    "P(c_1 \\mid x) = P(c_2 \\mid x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\">\n",
    "For multivariate case i.e., when $\\mathbf{x} \\in \\mathbb{R}^D$ then the decision function corresponds to a hyperplane. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is illustrated in Figure 3.2, where we assume that the conditional probability $P(c_k \\mid x)$ can be approximated by a continuous function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/figure-3.2.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Maximum Likelihood Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Decision Rule gives us a way to classify a sample $x$ into a class $c_k$ based on the conditional probability $P(c_k \\mid x)$. However, in the special case where the prior probability for all classes $P(c_k)$ are the same, then decision rule can be defined on class-conditional probability $p(x \\mid c_k)$ alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that? First, recall Equation 3.2 and Eq. 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/joint-probability.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/eq-3.6.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sample $x$, we can observe that its a priori probability $p(x)$ is a constant factor (given by Eq. 3.3) scaling the probability $P(c_k \\mid x)$ in the range of [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/eq-3.12.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar $\\alpha$ is:\n",
    "$$\n",
    "\\alpha = \\frac{P(c_k)}{p(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Equation 3.12, we can see that the probability of a class given an observation $x$ is proportional to the sample's class-conditional probability or likelihood i.e. $p(x\\mid c_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By classifying the new sample based on the highest conditional probability (using Bayes' Decision Rule), corresponds to classifying the sample based on its maximum likelihood. This process is referred to as **Maximum Likelihood Classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"summary\">\n",
    "In summary, Maximum Likelihood is a method to classify a sample based on the sample's likelihood $p(x \\mid c_k)$, which can be used when all classes have equal prior probability i.e. $P(c_k) = 1/K$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multi-variate Bayes' formula\n",
    "\n",
    "Until now we discussed about the case where the decision function is a function of only one continuous variable $x$. \n",
    "\n",
    "Let us consider the general case where the observations are more than one and are stored as a vector $\\mathbf{x}$. Then, the Bayes' formula is\n",
    "\n",
    "<img src=\"figures/lecture-17/bayes-formula-multi-d.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Risk-based decision functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classification errors are more important or have more impact than other. Suppose, we want to classify whether a tumor is malignant or benign. The impact of misclassifying malignant tumors is much higher than the misclassification of benign tumors. If our classifier determines that a tumor is benign when in fact it is not, then this may result in the patient dying because lack of treatment. However, misclassifying a tumor as malignant does not risk the patient's life. In situations like the above, we want consider the risk or the loss involved in taking some action $\\alpha_i$ based on the probability-based classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we observe a particular $\\mathbf{x}$ and that we contemplate taking action $\\alpha_i$. If the true class is different say $c_j$, then by definition we will incur some loss. We define the loss function $\\lambda(\\alpha_i \\mid c_j)$ describing the loss incurred by taking the action $\\alpha_i$ given that the correct class is $c_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"insight\">\n",
    "The loss function is an $K\\times K$ matrix given by the user.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the **risk** or the **expected loss** of taking action $\\alpha_i$ given the observation $\\mathbf{x}$  as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/conditional-risk.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, given an observation $\\mathbf{x}$ the best action is the one minimizing the risk. This means that we calculate the risk for each action $\\alpha_i$ and take action $\\alpha_l$ where $R(\\alpha_i \\mid \\mathbf{x})$ is lowest i.e. the action with the smallest risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total risk of a decision function is given by:\n",
    "\n",
    "$$\n",
    "\\sum_{\\mathbf{x}} p(\\mathbf{x}) R(\\alpha(\\mathbf{x}) \\mid \\mathbf{x}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision function is optimal if it minimises the total risk. The minimum overall risk is called **Bayes risk** and it corresponds to the best performance that can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Two-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider these results when applied to the special case of two-class classification problems. Suppose the action $\\alpha_1$ corresponds to deciding that the correct class is $c_1$ and the action $\\alpha_2$ corresponds to\n",
    "deciding that the correct class is $c_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/eq-3.16.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-17/likelihood-ratio.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
