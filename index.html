<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Main Page</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" media="screen" href="main.css" />
  
</head>
<body>
  
  <div class="container">
    <button class="toggle-all">Toggle all</button>
   
    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 1: Introduction to Optimization and Data Analytics </button>
      <div class="content">

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li>
                <a href="slides/01.0.pdf" target="_blank">Slide</a>
            </li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>No homework</li>
        </ul>
      </div>
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 2:  Matrix games and Linear programming 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Matrix games</li>
          <li>Geometric method</li>
          <li>Simplex method</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li><a href="slides/02.0.pdf" target="_blank">Slide: Matrix Games</a></li>
            <li><a href="slides/02.1.pdf" target="_blank">Slide: Geometric and simplex method</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>Homework in paper</li>
        </ul>
      </div>
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 3:  Simplex Method and Duality</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Simplex algorithm for a canonical linear programming problem</li>
          <li>Primal and dual problem</li>
          <li>The Duality Theorem</li>
          <li>Solution to the Dual Problem</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/03.0.pdf" target="_blank">Slide: Simplex Method and Duality</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec03_duality.html" target="_blank">Lecture 3: Duality</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw03_linear_programming_part2.html" target="_blank">Homework 3</a>
            <ul>
              <li>canonical linear programming problem</li>
              <li>geometric method</li>
              <li>convert minimisation problem to its dual problem</li>
              <li>convert investment problem to a matrix game</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 4: Basics of constrainted and unconstrained optimisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>The general optimisation problem</li>
          <li>Partial derivatives</li>
          <li>Hessian matrix</li>
          <li>Directional derivatives</li>
          <li>Level sets and contour plots</li>
          <li>Gradient is always orthogonal to the level set</li>
          <li>Feasible direction</li>
          <li>First Order Necessary Condition (FONC)</li>
          <li>Quadratic forms and extremal points</li>
          <li>Positive definite, negative definite and indifinite</li>
          <li>Determine definiteness of a quadratic form by its eigenvalues</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/04.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec04_general_optimization.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw04.html" target="_blank">Homework 4</a>
            <ul>
              <li>Draw level sets for functions</li>
              <li>Write down the Taylor series expansion</li>
              <li>Global minimiser for a reduced feasible set</li>
              <li>Directional derivative in the direction of maxmal rate of increase</li>
              <li>Find points that satisfy the FONC (interior case)</li>
              <li>Determine if points satisfy the SONC i.e., is the point a local minimiser?</li>
              <li>Argue wether a point is a local minimiser, a strict local minimiser, a local maximiser or a strict maximiser</li>
            </ul>
          </li>
        </ul>

        <h2 class="extra">Extra</h2>
        <ul class="extra">
          <li>
            <a href="extra/hessian-sonc.pdf" target="_blank">Hessian and saddle points</a>
          </li>
        </ul>
      </div>    
    </div>


    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 5: Problems with Equality Constraints</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Tangent space</li>
          <li>Normal space</li>
          <li>Langrange condition</li>
          <li>Langrange Condition in Support Vector Machines</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/05.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec05.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw05.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find local extremisers i.e., local minimisers and local maximisers for optimisation problems with equality constraints.</li>
              <li>Derive Lagrange condition</li>
              <li>Find values of the Lagrangian condition by hand</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 6: Solving Linear Systems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Systems of Linear Equations</li>
          <li>Matrix Norms</li>
          <li>Condition Numbers</li>
          <li>Relative residual</li>
          <li>Approximate inconsist linear system</li>
          <li>Least-squares problem</li>
          <li>Normal equation</li>
          <li>Formulate a problem with linear equations</li>
          <li>Line fitting using least-squares analysis</li>
          <li>Polynomial Regression through Least Square Method</li>
          <li>Recursive Least-Squares (RLS) Algorithm</li>
          <li>Solving Linear System with Minimum Norm</li>
          <li>Kaczmarz's Algorithm, iterative method for solving Ax=b</li>
          <li>Moore-Penrose inverse</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/06.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec06.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw06.html" target="_blank">Homework 5</a>
            <ul>
              <li>Line fitting: Approximate linear function given some data points</li>
              <li>Find a formula for estimating a factor using least-squares method</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 7: One-dimensional Search Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Unimodal functions</li>
          <li>Uncertainty interval reduction</li>
          <li>Golden Section Search (fixed Rho)</li>
          <li>Fibonacci Search (variable Rho)</li>
          <li>Bisection Search (split the interval in half)</li>
          <li>Newton's Search for minimisation and root finding</li>
          <li>Secant Search for minimisation and root finding
            <ul>
              <li>Used when derivative is not available</li>
              <li>Requires two points</li>
            </ul>
          </li>

        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/07.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec07.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw07.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find the number of iterations required for Golden Section Search</li>
              <li>Find minimiser using Golden Section Search </li>
              <li>Find minimiser using Fibonacci Search </li>
              <li>Plot 1D function to verify that it is unimodal over some interval</li>
              <li>Implement secant search</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 8: Gradient Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Steepest Descent, gradient-based algorithm</li>
          <li>Zig-Zag Behaviour of Steepest Descent</li>
          <li>Stopping Criteria</li>
          <li>Secant method to find largest alpha</li>
          <li>Quadratic forms and definiteness</li>
          <li>Eigen analysis, eigenvalues, eigenvectors</li>
          <li>Steepest Descent and Quadratic Functions
            <ul>
              <li>Finding an expression of alpha for Quadratic Forms</li>
            </ul>
          </li>
          <li>Convergence of Gradient Methods
            <ul>
              <li>The steepest descent algorithm will always converge for a quadratic form no matter where we start.</li>
            </ul>
          </li>
          <li>Fixed step-size Gradient Algorithm
            <ul>
              <li>Finding the range of alpha for which the fixed step-size gradient algorithm will converge.</li>
            </ul>
          </li>
          <li>Convergence Rate of Gradient Methods</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/08.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec08.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw08.html" target="_blank">Homework 8</a>
            <ul>
              <li>Find the largest range of alpha values for which a fixed-step-size gradient algorithm is globally convergent.</li>
              <li>Show that the least-squares function is a quadratic function</li>
              <li>Find the update rule for the fixed-step-size gradient algorithm for solving the least-squares problem</li>
              <li>Implement steepest descent algorithm</li>
              <li>Apply steepest descent algorithm on the Rosenbrock function</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>





    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 9: Newton's Method in Multivariate Optimisation Problems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Newton's Method or Newton-Raphson method for root finding and minimisation</li>
          <li>Relationship between root finding and minimisation</li>
          <li>How does Newton's method compare to steepest descent method</li>
          <li>Descent property of the Newton method</li>
          <li>Advantages and Disadvantages of Newton's Method</li>
          <li>Nonlinear Least Squares</li>
          <li>Curve fitting non-linear function using Levenberg-Marquardt algorithm</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/09.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec09.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw09.html" target="_blank">Homework 9</a>
            <ul>
              <li>The update rule in Newton's method for a one-dimensional problem</li>
              <li>Prove that an algorithm does not converge unless x=0</li>
              <li>Prove that [1,1] is the global minimiser for the Rosenbrock's Function</li>
              <li>Solve Rosenbrock's Function using Newton's method (works!)</li>
              <li>Solve Rosenbrock's Function using fixed-step-size gradient method (does not work!)</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>




    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 10: Newton's Method in Multivariate Optimisation Problems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Definiteness of Matrices</li>
          <li>Trust Region Algorithm</li>
          <li>Quasi-Newton
            <ul>
              <li>Replaces Jakobian (root finding) and Hessian (optimisation) with an approximation</li>
              <li>Avoids expensive computation</li>
              <li>Generalise the secant method</li>
              <li>Fast and more robust than Newton</li>
              <li>Quasi-Newton condition: B must satisfy a condition</li>
            </ul>
          </li>
          <li>Conjugate Direction Method
            <ul>
              <li>finds directions that are "conjugate"</li>
              <li>generating Q-conjugate directions are computationally expensive</li>
            </ul>
          </li>
          
          <li>Conjugate Gradient Method
            <ul>
              <li>incorporates the generation of Q-conjugate directions at each step</li>
              <li>avoids classic zigzag behaviour from steepest descent</li>
              <li>combine all these zigzags into one step per "direction"</li>
              <li>use the gradient to find these “conjugate” directions really efficiently (can even do this
                  incrementally as we go)</li>
            </ul>
          </li>
          <li>Gram-Schmidt process make linearly independent vectors orthogonal</li>
          <li>Conjugate Gradient vs Steepest Descent</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/10.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec10.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw10.html" target="_blank">Homework 10</a>
            <ul>
              <li>Contour plot</li>
              <li>Find a formula using the Q-conjugate method (<strong>study more</strong>)</li>
              <li>Express a function in a quadratic form</li>
              <li>Find minimiser of a quadratic function using conjugate gradient method</li>
              <li>Analytically find the minimiser of a quadratic function</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>





    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 11: Global Search Methods part 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Recap
            <ul>
              <li>What are Quasi-Newton methods? Replace Jakobian/Hessian in Newton's method with an approximation</li>
              <li>Difference between orthogonal and linearly independent? Linearly independent vectors do not necessarily have u*v=0</li>
              <li>Similarity between orthogonal and linearly independent? Both spans the same space</li>
              <li>What is A-conjugate? Pairs of vectors are A-orthogonal u^T Av=0</li>
              <li>What is definiteness? The bowl shape of a function</li>
              <li>Difference between steepest descent and conjugate gradient methods?
                Steepest descent always turns orthogonal.
                Conjugate gradient always turns A-orthogonal.
              </li>
            </ul>
          </li>
          <li>Global vs Local Optimum
            <ul>
              <li>A global optimum is not necessarily unique</li>
              <li>A saddle point is not a local optimum</li>
            </ul>
          </li>
          <li>Global vs Local Optimisation Algorithms
            <ul>
              <li>Global methods have built-in mechanisms to escape local minima</li>
              <li>Global methods do not gurantee to find the global optimum</li>
              <li>Local methods find local optima only (a local optimum may be global)</li>
              <li>Local methods have no way of escaping local minima</li>
              <li>Local methods tend to be fast</li>
            </ul>
            <li>Deterministic vs Stochastic algorithm</li>
            <li>Discrete vs Continous Optimisation
              <ul>
                <li>The input to the objective function determines whether the optimisation task is discrete or continuous.</li>
                <li>Solving the travelling salesman problem can be seen as a discrete optimisation task</li>
              </ul>
            </li>
            <li>Previous methods are all local methods
              <ul>
                <li>Gradient methods</li>
                <li>Newton's method</li>
                <li>Quasi-Newton methods</li>
                <li>Conjugate gradient methods</li>
                <li>These method start with an initial point and then generate a sequence of iterates.</li>
                <li>Typically, the generated sequence converges to a local minimizer.</li>
              </ul>
            </li>
            <li>Randomized search method or probabilistic search method</li>
            <li>Naive Random Search Algorithm
              <ul>
                <li>Issue 1: small neighbourhood results in inability escape local minima</li>
                <li>Issue 2: large neighbourhood results in slow search</li>
              </ul>
            </li>
            <li>Simulated annealing
              <ul>
                <li>Pick a worse candidate with some probability</li>
                <li>This probability should be high at the beginning but low towards the end</li>
                <li>Keep track of the best point so far</li>
              </ul>
            </li>
            <li>Particle Swarm Optimisation
              <ul>
                <li>Key difference: uses a set of candidate solutions called a swarm</li>
                <li>Each candidate solution is called a particle</li>
              </ul>
            </li>
            

        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/11.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec11.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw11.html" target="_blank">Homework 11</a>
            <ul>
              <li>Implement naive random search</li>
              <li>Implement simulated annealing</li>
              <li>Implement particle swarm optimisation</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>







    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 12: Genetic Algorithms</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Genetic algorithm structure
            <ul>
              <li>Pick suitable representation scheme, chromosome</li>
              <li>Generate initial population</li>
              <li>Evaluate the fitness of each individual's chromosome</li>
              <li>Create new population</li>
              <li>Selection via a mating pool</li>
              <li>Evolution by applying crossover (recombination) and mutation</li>
              <li>In-class exercise: turtle path finding</li>
              <li></li>
            </ul>
          </li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/12.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec12.html" target="_blank">Lecture notes</a></li>
        </ul>
  
        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw12.html" target="_blank">Homework 12</a>
            <ul>
              <li>Implement real-number genetic algorithm</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>
  
  
    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 13: Visualisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Recap: Why do genetic algorithms work?
            <ul>
              <li>Hypothesis: this process makes it very likely to find "building blocks" within the chromosomes that are:
                <ul>
                  <li>small, and</li>
                  <li>above average fitness</li>
                </ul>
              </li>
              <li>Building blocks are called "schema"</li>
              <li>The argument is that the genetic algorithm will automatically find the schema by following “selection-crossover-mutation” process
              <li>We do not need to identify the schema</li>
              <li>We do not get to see the schema</li>
              <li>We just get the candidate solutions</li>
            </ul>
          </li>
          <li>Visualisation helps us get insight into our
            <ul>
              <li>data set</li>
              <li>process of optimisation</li>
              <li>candidate solutions</li>
              <li>a model (think about curve fitting)</li>
            </ul>
          </li>
          <li>Visualisation helps us communicate insights to others</li>
          <li>Tools for visualisation
            <ul>
              <li>scatter plots</li>
              <li>scatter plot matrix (pairwise comparing two of the dimensions)</li>
              <li>parallel coordinates
                <ul>
                  <li>Using parallel coordinates to identify dependent or independent variables</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Dimensionality reduction
            <ul>
              <li>Combine some dimensions because they don't really help to distinguis the data</li>
              <li>Principle Component Analysis</li>
              <li>Want to capture as much spread/variance as possible</li>
            </ul>
          </li>
        </ul>
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/13.0.pdf" target="_blank">Slides</a></li>
        </ul>
      </div>    
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 14: Scientific Writing</button>
      <div class="content">
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/14.0.pdf" target="_blank">Slides</a></li>
        </ul>
      </div>    
    </div>
    

  </div>
 
  <script src="main.js"></script>
</body>
</html>