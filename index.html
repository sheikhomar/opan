<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Main Page</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" media="screen" href="main.css" />
  
</head>
<body>
  
  <div class="container">
    <button class="toggle-all">Toggle all</button>
   
    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 1: Introduction to Optimization and Data Analytics </button>
      <div class="content">

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li>
                <a href="slides/01.0.pdf" target="_blank">Slide</a>
            </li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>No homework</li>
        </ul>
      </div>
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 2:  Matrix games and Linear programming 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Matrix games</li>
          <li>Geometric method</li>
          <li>Simplex method</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li><a href="slides/02.0.pdf" target="_blank">Slide: Matrix Games</a></li>
            <li><a href="slides/02.1.pdf" target="_blank">Slide: Geometric and simplex method</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>Homework in paper</li>
        </ul>
      </div>
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 3:  Simplex Method and Duality</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Simplex algorithm for a canonical linear programming problem</li>
          <li>Primal and dual problem</li>
          <li>The Duality Theorem</li>
          <li>Solution to the Dual Problem</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/03.0.pdf" target="_blank">Slide: Simplex Method and Duality</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec03_duality.html" target="_blank">Lecture 3: Duality</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw03_linear_programming_part2.html" target="_blank">Homework 3</a>
            <ul>
              <li>canonical linear programming problem</li>
              <li>geometric method</li>
              <li>convert minimisation problem to its dual problem</li>
              <li>convert investment problem to a matrix game</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 4: Basics of constrainted and unconstrained optimisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>The general optimisation problem</li>
          <li>Partial derivatives</li>
          <li>Hessian matrix</li>
          <li>Directional derivatives</li>
          <li>Level sets and contour plots</li>
          <li>Gradient is always orthogonal to the level set</li>
          <li>Feasible direction</li>
          <li>First Order Necessary Condition (FONC)</li>
          <li>Quadratic forms and extremal points</li>
          <li>Positive definite, negative definite and indifinite</li>
          <li>Determine definiteness of a quadratic form by its eigenvalues</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/04.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec04_general_optimization.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw04.html" target="_blank">Homework 4</a>
            <ul>
              <li>Draw level sets for functions</li>
              <li>Write down the Taylor series expansion</li>
              <li>Global minimiser for a reduced feasible set</li>
              <li>Directional derivative in the direction of maxmal rate of increase</li>
              <li>Find points that satisfy the FONC (interior case)</li>
              <li>Determine if points satisfy the SONC i.e., is the point a local minimiser?</li>
              <li>Argue wether a point is a local minimiser, a strict local minimiser, a local maximiser or a strict maximiser</li>
            </ul>
          </li>
        </ul>

        <h2 class="extra">Extra</h2>
        <ul class="extra">
          <li>
            <a href="extra/hessian-sonc.pdf" target="_blank">Hessian and saddle points</a>
          </li>
        </ul>
      </div>    
    </div>


    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 5: Problems with Equality Constraints</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Tangent space</li>
          <li>Normal space</li>
          <li>Langrange condition</li>
          <li>Langrange Condition in Support Vector Machines</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/05.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec05.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw05.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find local extremisers i.e., local minimisers and local maximisers for optimisation problems with equality constraints.</li>
              <li>Derive Lagrange condition</li>
              <li>Find values of the Lagrangian condition by hand</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 6: Solving Linear Systems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Systems of Linear Equations</li>
          <li>Matrix Norms</li>
          <li>Condition Numbers</li>
          <li>Relative residual</li>
          <li>Approximate inconsist linear system</li>
          <li>Least-squares problem</li>
          <li>Normal equation</li>
          <li>Formulate a problem with linear equations</li>
          <li>Line fitting using least-squares analysis</li>
          <li>Polynomial Regression through Least Square Method</li>
          <li>Recursive Least-Squares (RLS) Algorithm</li>
          <li>Solving Linear System with Minimum Norm</li>
          <li>Kaczmarz's Algorithm, iterative method for solving Ax=b</li>
          <li>Moore-Penrose inverse</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/06.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec06.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw06.html" target="_blank">Homework 5</a>
            <ul>
              <li>Line fitting: Approximate linear function given some data points</li>
              <li>Find a formula for estimating a factor using least-squares method</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 7: One-dimensional Search Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Unimodal functions</li>
          <li>Uncertainty interval reduction</li>
          <li>Golden Section Search (fixed Rho)</li>
          <li>Fibonacci Search (variable Rho)</li>
          <li>Bisection Search (split the interval in half)</li>
          <li>Newton's Search for minimisation and root finding</li>
          <li>Secant Search for minimisation and root finding
            <ul>
              <li>Used when derivative is not available</li>
              <li>Requires two points</li>
            </ul>
          </li>

        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/07.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec07.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw07.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find the number of iterations required for Golden Section Search</li>
              <li>Find minimiser using Golden Section Search </li>
              <li>Find minimiser using Fibonacci Search </li>
              <li>Plot 1D function to verify that it is unimodal over some interval</li>
              <li>Implement secant search</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>



    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 8: Gradient Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Steepest Descent, gradient-based algorithm</li>
          <li>Zig-Zag Behaviour of Steepest Descent</li>
          <li>Stopping Criteria</li>
          <li>Secant method to find largest alpha</li>
          <li>Quadratic forms and definiteness</li>
          <li>Eigen analysis, eigenvalues, eigenvectors</li>
          <li>Steepest Descent and Quadratic Functions
            <ul>
              <li>Finding an expression of alpha for Quadratic Forms</li>
            </ul>
          </li>
          <li>Convergence of Gradient Methods
            <ul>
              <li>The steepest descent algorithm will always converge for a quadratic form no matter where we start.</li>
            </ul>
          </li>
          <li>Fixed step-size Gradient Algorithm
            <ul>
              <li>Finding the range of alpha for which the fixed step-size gradient algorithm will converge.</li>
            </ul>
          </li>
          <li>Convergence Rate of Gradient Methods</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/08.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec08.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw08.html" target="_blank">Homework 8</a>
            <ul>
              <li>Find the largest range of alpha values for which a fixed-step-size gradient algorithm is globally convergent.</li>
              <li>Show that the least-squares function is a quadratic function</li>
              <li>Find the update rule for the fixed-step-size gradient algorithm for solving the least-squares problem</li>
              <li>Implement steepest descent algorithm</li>
              <li>Apply steepest descent algorithm on the Rosenbrock function</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>





    <!-- LECTURE SECTION -->
    <div class="lecture">
      <button class="collapsible">Lecture 9: Newton's Method in Multivariate Optimisation Problems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Newton's Method or Newton-Raphson method for root finding and minimisation</li>
          <li>Relationship between root finding and minimisation</li>
          <li>How does Newton's method compare to steepest descent method</li>
          <li>Descent property of the Newton method</li>
          <li>Advantages and Disadvantages of Newton's Method</li>
          <li>Nonlinear Least Squares</li>
          <li>Curve fitting non-linear function using Levenberg-Marquardt algorithm</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/09.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec09.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw09.html" target="_blank">Homework 9</a>
            <ul>
              <li>The update rule in Newton's method for a one-dimensional problem</li>
              <li>Prove that an algorithm does not converge unless x=0</li>
              <li>Prove that [1,1] is the global minimiser for the Rosenbrock's Function</li>
              <li>Solve Rosenbrock's Function using Newton's method (works!)</li>
              <li>Solve Rosenbrock's Function using fixed-step-size gradient method (does not work!)</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>


  </div>
 
  <script src="main.js"></script>
</body>
</html>