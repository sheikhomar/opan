<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Main Page</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" media="screen" href="main.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>  

  <!-- Loading mathjax macro -->
  <!-- Load mathjax -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
  <!-- MathJax configuration -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true,
          processEnvironments: true
      },
      // Center justify equations in code and markdown cells. Elsewhere
      // we use CSS to left justify single line equations in code cells.
      displayAlign: 'center',
      "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 0}},
          linebreaks: { automatic: true }
      }
  });
  </script>
  <!-- End of mathjax configuration -->
</head>
<body>

  <div class="sidebar">
    <ul>
      <li> <a href="extra/latex-symbols-00.pdf" target="_blank">The Comprehensive LaTeX Symbol List</a></li>
      <li> <a href="extra/latex-symbols-01.pdf" target="_blank">List of LaTeX mathematical symbols</a></li>
      <li> <a href="extra/latex-symbols-02.pdf" target="_blank">List of Greek letters and math symbols</a></li>
      <li> <a href="extra/latex-symbols-03.pdf" target="_blank">LaTeX math symbols</a></li>
    </ul>
  </div>
  
  <div class="container">
    <button class="toggle-all">Toggle all</button>

    <div class="lecture">
      <button class="collapsible">Lecture 1: Introduction to Optimization and Data Analytics </button>
      <div class="content">

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li>
                <a href="slides/01.0.pdf" target="_blank">Slide</a>
            </li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>No homework</li>
        </ul>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 2:  Matrix games and Linear programming 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Matrix games</li>
          <li>Geometric method</li>
          <li>Simplex method</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
            <li><a href="slides/02.0.pdf" target="_blank">Slide: Matrix Games</a></li>
            <li><a href="slides/02.1.pdf" target="_blank">Slide: Geometric and simplex method</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>Homework in paper</li>
        </ul>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 3:  Simplex Method and Duality</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Simplex algorithm for a canonical linear programming problem</li>
          <li>Primal and dual problem</li>
          <li>The Duality Theorem</li>
          <li>Solution to the Dual Problem</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/03.0.pdf" target="_blank">Slide: Simplex Method and Duality</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec03_duality.html" target="_blank">Lecture 3: Duality</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw03_linear_programming_part2.html" target="_blank">Homework 3</a>
            <ul>
              <li>canonical linear programming problem</li>
              <li>geometric method</li>
              <li>convert minimisation problem to its dual problem</li>
              <li>convert investment problem to a matrix game</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 4: Basics of constrainted and unconstrained optimisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>The general optimisation problem</li>
          <li>Partial derivatives</li>
          <li>Hessian matrix</li>
          <li>Directional derivatives</li>
          <li>Level sets and contour plots</li>
          <li>Gradient is always orthogonal to the level set</li>
          <li>Feasible direction</li>
          <li>First Order Necessary Condition (FONC)</li>
          <li>Quadratic forms and extremal points</li>
          <li>Positive definite, negative definite and indifinite</li>
          <li>Determine definiteness of a quadratic form by its eigenvalues</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/04.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec04_general_optimization.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw04.html" target="_blank">Homework 4</a>
            <ul>
              <li>Draw level sets for functions</li>
              <li>Write down the Taylor series expansion</li>
              <li>Global minimiser for a reduced feasible set</li>
              <li>Directional derivative in the direction of maxmal rate of increase</li>
              <li>Find points that satisfy the FONC (interior case)</li>
              <li>Determine if points satisfy the SONC i.e., is the point a local minimiser?</li>
              <li>Argue wether a point is a local minimiser, a strict local minimiser, a local maximiser or a strict maximiser</li>
            </ul>
          </li>
        </ul>

        <h2 class="extra">Extra</h2>
        <ul class="extra">
          <li>
            <a href="extra/hessian-sonc.pdf" target="_blank">Hessian and saddle points</a>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 5: Problems with Equality Constraints</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Tangent space</li>
          <li>Normal space</li>
          <li>Langrange condition</li>
          <li>Langrange Condition in Support Vector Machines</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/05.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec05.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw05.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find local extremisers i.e., local minimisers and local maximisers for optimisation problems with equality constraints.</li>
              <li>Derive Lagrange condition</li>
              <li>Find values of the Lagrangian condition by hand</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 6: Solving Linear Systems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Systems of Linear Equations</li>
          <li>Matrix Norms</li>
          <li>Condition Numbers</li>
          <li>Relative residual</li>
          <li>Approximate inconsist linear system</li>
          <li>Least-squares problem</li>
          <li>Normal equation</li>
          <li>Formulate a problem with linear equations</li>
          <li>Line fitting using least-squares analysis</li>
          <li>Polynomial Regression through Least Square Method</li>
          <li>Recursive Least-Squares (RLS) Algorithm</li>
          <li>Solving Linear System with Minimum Norm</li>
          <li>Kaczmarz's Algorithm, iterative method for solving Ax=b</li>
          <li>Moore-Penrose inverse</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/06.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec06.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw06.html" target="_blank">Homework 5</a>
            <ul>
              <li>Line fitting: Approximate linear function given some data points</li>
              <li>Find a formula for estimating a factor using least-squares method</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 7: One-dimensional Search Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Unimodal functions</li>
          <li>Uncertainty interval reduction</li>
          <li>Golden Section Search (fixed Rho)</li>
          <li>Fibonacci Search (variable Rho)</li>
          <li>Bisection Search (split the interval in half)</li>
          <li>Newton's Search for minimisation and root finding</li>
          <li>Secant Search for minimisation and root finding
            <ul>
              <li>Used when derivative is not available</li>
              <li>Requires two points</li>
            </ul>
          </li>

        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/07.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec07.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw07.html" target="_blank">Homework 5</a>
            <ul>
              <li>Find the number of iterations required for Golden Section Search</li>
              <li>Find minimiser using Golden Section Search </li>
              <li>Find minimiser using Fibonacci Search </li>
              <li>Plot 1D function to verify that it is unimodal over some interval</li>
              <li>Implement secant search</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 8: Gradient Methods</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Steepest Descent, gradient-based algorithm</li>
          <li>Zig-Zag Behaviour of Steepest Descent</li>
          <li>Stopping Criteria</li>
          <li>Secant method to find largest alpha</li>
          <li>Quadratic forms and definiteness</li>
          <li>Eigen analysis, eigenvalues, eigenvectors</li>
          <li>Steepest Descent and Quadratic Functions
            <ul>
              <li>Finding an expression of alpha for Quadratic Forms</li>
            </ul>
          </li>
          <li>Convergence of Gradient Methods
            <ul>
              <li>The steepest descent algorithm will always converge for a quadratic form no matter where we start.</li>
            </ul>
          </li>
          <li>Fixed step-size Gradient Algorithm
            <ul>
              <li>Finding the range of alpha for which the fixed step-size gradient algorithm will converge.</li>
            </ul>
          </li>
          <li>Convergence Rate of Gradient Methods</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/08.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec08.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw08.html" target="_blank">Homework 8</a>
            <ul>
              <li>Find the largest range of alpha values for which a fixed-step-size gradient algorithm is globally convergent.</li>
              <li>Show that the least-squares function is a quadratic function</li>
              <li>Find the update rule for the fixed-step-size gradient algorithm for solving the least-squares problem</li>
              <li>Implement steepest descent algorithm</li>
              <li>Apply steepest descent algorithm on the Rosenbrock function</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 9: Newton's Method in Multivariate Optimisation Problems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Newton's Method or Newton-Raphson method for root finding and minimisation</li>
          <li>Relationship between root finding and minimisation</li>
          <li>How does Newton's method compare to steepest descent method</li>
          <li>Descent property of the Newton method</li>
          <li>Advantages and Disadvantages of Newton's Method</li>
          <li>Nonlinear Least Squares</li>
          <li>Curve fitting non-linear function using Levenberg-Marquardt algorithm</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/09.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec09.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw09.html" target="_blank">Homework 9</a>
            <ul>
              <li>The update rule in Newton's method for a one-dimensional problem</li>
              <li>Prove that an algorithm does not converge unless x=0</li>
              <li>Prove that [1,1] is the global minimiser for the Rosenbrock's Function</li>
              <li>Solve Rosenbrock's Function using Newton's method (works!)</li>
              <li>Solve Rosenbrock's Function using fixed-step-size gradient method (does not work!)</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 10: Newton's Method in Multivariate Optimisation Problems</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Definiteness of Matrices</li>
          <li>Trust Region Algorithm</li>
          <li>Quasi-Newton
            <ul>
              <li>Replaces Jakobian (root finding) and Hessian (optimisation) with an approximation</li>
              <li>Avoids expensive computation</li>
              <li>Generalise the secant method</li>
              <li>Fast and more robust than Newton</li>
              <li>Quasi-Newton condition: B must satisfy a condition</li>
            </ul>
          </li>
          <li>Conjugate Direction Method
            <ul>
              <li>finds directions that are "conjugate"</li>
              <li>generating Q-conjugate directions are computationally expensive</li>
            </ul>
          </li>
          
          <li>Conjugate Gradient Method
            <ul>
              <li>incorporates the generation of Q-conjugate directions at each step</li>
              <li>avoids classic zigzag behaviour from steepest descent</li>
              <li>combine all these zigzags into one step per "direction"</li>
              <li>use the gradient to find these “conjugate” directions really efficiently (can even do this
                  incrementally as we go)</li>
            </ul>
          </li>
          <li>Gram-Schmidt process make linearly independent vectors orthogonal</li>
          <li>Conjugate Gradient vs Steepest Descent</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/10.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec10.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw10.html" target="_blank">Homework 10</a>
            <ul>
              <li>Contour plot</li>
              <li>Find a formula using the Q-conjugate method (<strong>study more</strong>)</li>
              <li>Express a function in a quadratic form</li>
              <li>Find minimiser of a quadratic function using conjugate gradient method</li>
              <li>Analytically find the minimiser of a quadratic function</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 11: Global Search Methods part 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Recap
            <ul>
              <li>What are Quasi-Newton methods? Replace Jakobian/Hessian in Newton's method with an approximation</li>
              <li>Difference between orthogonal and linearly independent? Linearly independent vectors do not necessarily have u*v=0</li>
              <li>Similarity between orthogonal and linearly independent? Both spans the same space</li>
              <li>What is A-conjugate? Pairs of vectors are A-orthogonal u^T Av=0</li>
              <li>What is definiteness? The bowl shape of a function</li>
              <li>Difference between steepest descent and conjugate gradient methods?
                Steepest descent always turns orthogonal.
                Conjugate gradient always turns A-orthogonal.
              </li>
            </ul>
          </li>
          <li>Global vs Local Optimum
            <ul>
              <li>A global optimum is not necessarily unique</li>
              <li>A saddle point is not a local optimum</li>
            </ul>
          </li>
          <li>Global vs Local Optimisation Algorithms
            <ul>
              <li>Global methods have built-in mechanisms to escape local minima</li>
              <li>Global methods do not gurantee to find the global optimum</li>
              <li>Local methods find local optima only (a local optimum may be global)</li>
              <li>Local methods have no way of escaping local minima</li>
              <li>Local methods tend to be fast</li>
            </ul>
            <li>Deterministic vs Stochastic algorithm</li>
            <li>Discrete vs Continous Optimisation
              <ul>
                <li>The input to the objective function determines whether the optimisation task is discrete or continuous.</li>
                <li>Solving the travelling salesman problem can be seen as a discrete optimisation task</li>
              </ul>
            </li>
            <li>Previous methods are all local methods
              <ul>
                <li>Gradient methods</li>
                <li>Newton's method</li>
                <li>Quasi-Newton methods</li>
                <li>Conjugate gradient methods</li>
                <li>These method start with an initial point and then generate a sequence of iterates.</li>
                <li>Typically, the generated sequence converges to a local minimizer.</li>
              </ul>
            </li>
            <li>Randomized search method or probabilistic search method</li>
            <li>Naive Random Search Algorithm
              <ul>
                <li>Issue 1: small neighbourhood results in inability escape local minima</li>
                <li>Issue 2: large neighbourhood results in slow search</li>
              </ul>
            </li>
            <li>Simulated annealing
              <ul>
                <li>Pick a worse candidate with some probability</li>
                <li>This probability should be high at the beginning but low towards the end</li>
                <li>Keep track of the best point so far</li>
              </ul>
            </li>
            <li>Particle Swarm Optimisation
              <ul>
                <li>Key difference: uses a set of candidate solutions called a swarm</li>
                <li>Each candidate solution is called a particle</li>
              </ul>
            </li>
            

        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/11.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec11.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw11.html" target="_blank">Homework 11</a>
            <ul>
              <li>Implement naive random search</li>
              <li>Implement simulated annealing</li>
              <li>Implement particle swarm optimisation</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 12: Genetic Algorithms</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Genetic algorithm structure
            <ul>
              <li>Pick suitable representation scheme, chromosome</li>
              <li>Generate initial population</li>
              <li>Evaluate the fitness of each individual's chromosome</li>
              <li>Create new population</li>
              <li>Selection via a mating pool</li>
              <li>Evolution by applying crossover (recombination) and mutation</li>
              <li>In-class exercise: turtle path finding</li>
              <li></li>
            </ul>
          </li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/12.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec12.html" target="_blank">Lecture notes</a></li>
        </ul>
  
        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw12.html" target="_blank">Homework 12</a>
            <ul>
              <li>Implement real-number genetic algorithm</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>
  
    <div class="lecture">
      <button class="collapsible">Lecture 13: Visualisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Recap: Why do genetic algorithms work?
            <ul>
              <li>Hypothesis: this process makes it very likely to find "building blocks" within the chromosomes that are:
                <ul>
                  <li>small, and</li>
                  <li>above average fitness</li>
                </ul>
              </li>
              <li>Building blocks are called "schema"</li>
              <li>The argument is that the genetic algorithm will automatically find the schema by following “selection-crossover-mutation” process
              <li>We do not need to identify the schema</li>
              <li>We do not get to see the schema</li>
              <li>We just get the candidate solutions</li>
            </ul>
          </li>
          <li>Visualisation helps us get insight into our
            <ul>
              <li>data set</li>
              <li>process of optimisation</li>
              <li>candidate solutions</li>
              <li>a model (think about curve fitting)</li>
            </ul>
          </li>
          <li>Visualisation helps us communicate insights to others</li>
          <li>Tools for visualisation
            <ul>
              <li>scatter plots</li>
              <li>scatter plot matrix (pairwise comparing two of the dimensions)</li>
              <li>parallel coordinates
                <ul>
                  <li>Using parallel coordinates to identify dependent or independent variables</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Dimensionality reduction
            <ul>
              <li>Combine some dimensions because they don't really help to distinguis the data</li>
              <li>Principle Component Analysis</li>
              <li>Want to capture as much spread/variance as possible</li>
            </ul>
          </li>
        </ul>
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/13.0.pdf" target="_blank">Slides</a></li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 14: Scientific Writing</button>
      <div class="content">
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/14.0.pdf" target="_blank">Slides</a></li>
        </ul>
      </div>    
    </div>
    
    <div class="lecture">

      <button class="collapsible">Lecture 15:  Introduction to Data Analytics</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Data Analytics is the discovery, interpretation and communication of meaningful patterns in data</li>
          <li>Empirical error is error in training data</li>
          <li>Expected Error the error in test data</li>
          <li>Data pre-processing</li>
          <li>Data representation</li>
          <li>Representation pre-processing
            <ul>
              <li>Dimensionality reduction</li>
              <li>Data centering</li>
              <li>Data standardisation</li>
              <li>Data normalisation</li>
            </ul>
          </li>
          <li>Model selection/training
            <ul>
              <li>Define what technique will be used (e.g. linear classifier, nonlinear classifier, etc.)</li>
              <li>Defining the ‘optimal parameters’ of the model.</li>
            </ul>
          </li>
          <li>Categorisation of machine learning models
            <ul>
              <li>Supervised Learning: traning data is labelled</li>
              <li>Unsupervised Learning: no labels, assumptions that data naturally form groups</li>
              <li>Reinforcement Learning: training is guided by an expert who provide only binary feedback. Extremely slow training process!</li>
            </ul>
          </li>
          

        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/15.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="homework">No homework</h2>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 16: Unsupervised Learning</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Data pre-processing: Formatting, cleaning, cleaning, sampling, scaling</li>
          <li>Data centering: translate the mean of each dimension to zero</li>
          <li>Data standardisation, normalisation, rotation</li>
          <li>Distance metrics: Manhattan and Euclidean </li>
          <li>Similarity measures and distance-based similary functions</li>
          <li>Principle Component Analysis
            <ul>
              <li>Automatically select number of components in PCA</li>
            </ul>
          </li>
          <li>Clustering: finds groups of data</li>
          <li>K-Means algorithm</li>
          <li>Fuzzy k-Means</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/16.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec16.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw16.html" target="_blank">Homework 16</a>
            <ul>
              <li>Prove that the projection matrix in PCA solves the regression problem</li>
              <li>Apply three iterations of the batch k-Means algorithm</li>
              <li>Plot data points before and after applying k-Means</li>
              <li>Apply fuzzy k-Means algorithm</li>
              <li>Perform data centering given 2D data points</li>
              <li>Plot data points before and after centering</li>
              <li>Perform data standardisation</li>
              <li>Perform data normalisation using l2 norm</li>
              <li>Project 2D data to a 1D space using PCA</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 17: Bayesian Decision Theory </button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Probability mass function, P(), used for discrete variables</li>
          <li>Probability density function, p(), used for continuous variables</li>
          <li>Prior probability, conditional, class-conditional probability</li>
          <li>Joint probability</li>
          <li>Bayes' formula</li>
          <li>Bayes' decision rule</li>
          <li>Probability of error for justifying Bayes' decision rule</li>
          <li>Decision functions for class-conditional probabilities</li>
          <li>Maximum Likelihood Classification</li>
          <li>Multivariate Bayes' formula</li>
          <li>Risk-based decision functions</li>
          <li>Risk-based Classification</li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/17.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec17.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">No homework</h2>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 16: Unsupervised Learning</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Data pre-processing: Formatting, cleaning, cleaning, sampling, scaling</li>
          <li>Data centering: translate the mean of each dimension to zero</li>
          <li>Data standardisation, normalisation, rotation</li>
          <li>Distance metrics: Manhattan and Euclidean </li>
          <li>Similarity measures and distance-based similary functions</li>
          <li>Principle Component Analysis
            <ul>
              <li>Automatically select number of components in PCA</li>
            </ul>
          </li>
          <li>Clustering: finds groups of data</li>
          <li>K-Means algorithm</li>
          <li>Fuzzy k-Means</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/16.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec16.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw16.html" target="_blank">Homework 16</a>
            <ul>
              <li>Prove that the projection matrix in PCA solves the regression problem</li>
              <li>Apply three iterations of the batch k-Means algorithm</li>
              <li>Plot data points before and after applying k-Means</li>
              <li>Apply fuzzy k-Means algorithm</li>
              <li>Perform data centering given 2D data points</li>
              <li>Plot data points before and after centering</li>
              <li>Perform data standardisation</li>
              <li>Perform data normalisation using l2 norm</li>
              <li>Project 2D data to a 1D space using PCA</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 18: Maximum Likelihood</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Decision functions of the normal density</li>
          <li>Multivariate normal distribution</li>
          <li>Covariance matrix defines important properties of the distribution
            <ul>
              <li>If an off-diagonal entry $a_{ij}, i \neq j$ is 0 then $i$ and $j$ are statistically independent</li>
              <li>Mahanolobis Distance takes into account the scaling of each dimension and their co-variances</li>
              <li>Euclidean distance assumes each dimension have equal weight i.e., the dimensions are independent</li>
            </ul>
          </li>
          <li>Data whitening
            <ul>
              <li>Transformation $\mathbf{X} \to \mathbf{Y}$ such that the covariance of $\mathbf{Y}$ is the identity</li>
              <li>Why 1? Because there are no correlation between the dimensions i.e., each dimension is statistically independent</li>
              <li>Why 2? Because computation is easier when covariance matrix is the identity</li>
            </ul>
          </li>
          <li>Use of monotonic function to get rid of exponential function</li>
          <li>Maximum Likelihood Estimation
            <ul>
              <li>Assume that $p(x|c_k) \sim$ follows a Normal Distribution with mean
                  vector μk and covariance matrix Σk.</li>
              <li>Assume that the distribution of each class is independent of those of the other classes.</li>
              <li>We want to find the set of model parameters which gives the highest
                possible likelihood given the data.</li>
            </ul>
          </li>
          <li>Expectation-Maximization (EM): two step iterative algorithm
            <ul>
              <li>Expectation step calculates the expected labels</li>
              <li>Maximisation step miximises the conditional probabilities </li>
            </ul>
          </li>
          <li>Relation between k-Mean and Expectation-Maximization</li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/18.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">No Lecture Notes</h2>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw18.html" target="_blank">Homework 18</a>
            <ul>
              <li>Classifiy new samples given two classes of training samples using Bayes' Decision Rule</li>
              <li>Risk-based classification</li>
              <li>Classification for samples that follow normal distribution (given covariance matrix)</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 19: Non-Parametric Techniques</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Nearest Prototype Classification</li>
          <li>Nearest Class Centroid (NCC)</li>
          <li>Probability-based Classifier vs Nearest Centroid Classifier</li>
          <li>Nearest Sub-Class Centroid (NSC), creates non-linear decision boundary</li>
          <li>Nearest Neighbour, best classifier given large enough dataset</li>
          <li>Fisher Discriminant Analysis
            <ul>
              <li>Assumes that samples in each class is unimodal and follows a normal distribution</li>
              <li>Objective 1: Maximise the distance between mean values of each class in the projective subspace</li>
              <li>Objective 2: Minimise the variance within each class in the projected subspace</li>
              <li>Fisher's ratio optimises both objectives simultaneously</li>
              <li>Solution obtained by eigenanalysis</li>
            </ul>
          </li>
          <li>Linear Discriminant Analysis, problems with more than two classes</li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/19.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec19.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">No homework</h2>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 20: Linear Decision Functions</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Bias term expresses the displacement of the hyperplane from the origin to one of the dimensions</li>
          <li>The weight vector $\mathbf{w}$ is the normal to the discriminant hyperplane</li>
          <li>Augmented vectors</li>
          <li>Express two discriminant functions as one expression</li>
          <li>Perceptron criterion function uses misclassified samples</li>
          <li>Batch Perceptron algorithm</li>
          <li>Single-sample Perceptron, update $\mathbf{w}$ based on a single randomly selected sample</li>
          <li>Single-sample Perceptron with margin
            <ul>
              <li>Margin is a hyperparameter of the algorithm (grid search to find a good margin)</li>
            </ul>
          </li>
          <li>Batch Relaxation with Margin</li>
          <li>Single-sample Relaxation with Margin</li>
          <li>How to select the learning rate?</li>
          <li>Is sample-based update better than batch update? Depends, typically we use mini-batches.</li>
          <li>Use of Linear Programming to solve the Perceptron objective function
            <ul>
              <li>Convert the problem to a linear programming</li>
            </ul>
          </li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/20.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">No lecture Notes</h2>

        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw20.html" target="_blank">Homework 20</a>
            <ul>
              <li>Prove that maximising the LDA criterion is equivalent to minimising another criterion.</li>
              <li>Classify new samples using Nearest Centroid classifier</li>
              <li>Classify new samples using Nearest Neighbour classifier</li>
              <li>Calculate the data projections in the 1-dimensional feature space determined by applying Fisher Discriminant Analysis</li>
              <li>Visualisation of projections using Fisher Discriminant Analysis</li>
              <li>Train a Perceptron and classify new samples using the trained model</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>


    <div class="lecture">
      <button class="collapsible">Lecture 21: Generalized Linear Discriminant Functions</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Perceptron objective function based on the least-squares problem</li>
          <li>Why use l2 norm in the objective function?
            <ul>
              <li>We get a quadratic objective function with a single minimum</li>
              <li>The use of squared makes it easier to compute the derivative</li>
              <li>The use of the l2 norm in the objective function allows to express our objective in a single value</li>
            </ul>
          </li>
          <li>A matrix is invertible (non-singular) if it has full rank</li>
          <li>What full rank means in machine learning</li>
          <li>When a matrix is not invertible then we can use the pseudo-inverse</li>
          <li>Solving the least-squares problem analytically</li>
          <li>Solving the least-squares problem iteratively</li>
          <li>How to obtain non-linear decision functions
            <ul>
              <li>Change the form of the decision function by adding terms involving products. Not used in practice!</li>
              <li>Change the data representation by defining a nonlinear mapping $\phi(\cdot)$</li>
            </ul>
          </li>
          <li>Binary classifiers in multi-class classification problems
            <ul>
              <li>One-versus-Rest: Train $K$ binary classifiers</li>
              <li>One-versus-One: classify based on majority voting</li>
            </ul>
          </li>
        </ul>

        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/21.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec21.html" target="_blank">Lecture notes</a></li>
        </ul>

        <h2 class="homework">No homework</h2>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 22: Kernel-based Learning</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Support Vector Machine
            <ul>
              <li>Infinite decision functions possible to separate samples of two classes</li>
              <li>Discriminant hyperplane with large margin</li>
              <li>Kernel function generates nonlinear decision function</li>
              <li>Slack variables allows samples to violate the margin</li>
              <li>Objective function solved by defining the Lagrangian</li>
              <li>SVM objective function is turned into quadratic convex optimisation problem</li>
              <li>Primal and Dual problem of SVM</li>
              <li></li>
            </ul>
          </li>
          <li>Kernel functions
            <ul>
              <li>Kernel function generates a positive semi-definite matrix called Kernel Matrix</li>
              <li>Elements of the kernel matrix are defined on pairs of samples</li>
              <li>Radial Basis Function (RBF) kernel function</li>
              <li>Polynomial kernel function</li>
            </ul>
          </li>
          <li>Kernel Least-Means Square Regression
            <ul>
              <li>Linear regression using kernel mapped data</li>
              <li>Representer Theorem to substitute $W$ </li>
            </ul>
          </li>
          <li>Kernel Discriminant Analysis</li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/22.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
        <ul class="lecture-notes">
          <li><a href="./lec22.html" target="_blank">Lecture notes</a></li>
        </ul>
  
        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw22.html" target="_blank">Homework 22 (mostly not done)</a>
            <ul>
              <li>Prove that the decision regions for a multi-class classifier with $K$ linear discriminant function are convex</li>
              <li>There always exists a nonlinear mapping to a higher dimension that makes samples linearly separable</li>
              <li>Suppose that each training sample $\mathbf{x}_i$ in class 1 has a symmetric point in class 2 equal to $-\mathbf{x}_i$.
                  Prove that the separating hyper of Least-Mean Square LMS solution passes through the origin</li>
              <li>Plot decision functions for Support Vector Machine SVM classifier, draw optimal margin and support vectors</li>
              <li>Show that the kernel matrix $\mathbf{K}$ in the dual problem formulation of SVM must be a positive definite in order to find a solution</li>
              <li>Show that the linear SVM classifier always finds the optimal hyper-plane in the linear case.</li>
              <li>How can kernel discriminant analysis be extended to exploit any type of pair-wise relationships</li>
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 23: Overfitting, hyperparameter selection and regularisation</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Training phase and test phase</li>
          <li>Empirical loss: error on the training set</li>
          <li>Expected loss: error on the test set</li>
          <li>Generalisation ability is measured using previously unseen samples</li>
          <li>Assumes that the unseen samples are drawn from the same distribution as the training data i.e., samples from both sets exhibit similar properties</li>
          <li>Underfitting: the model parameters are two few to represent the underlying process that generated the samples
            <ul>
              <li>The model can capture "trends" in the data</li>
              <li>The model cannot capture details</li>
            </ul>
          </li>
          <li>Good fit means that model is
            <ul>
              <li>simple enough to capture trends in the data, and</li>
              <li>complex enough to capture details</li>
            </ul>
          </li>
          <li>Overfitting: the model parameters are too many
            <ul>
              <li>The model captures details (however small) and deviates from the global trends</li>
              <li>The model just memorises the data but does not learn the underlying process that generate the samples</li>
            </ul>
          </li>
          <li>The <strong>capacity</strong> of a model is the maximum number of
              samples it can memorize (irrespectively of their arrangement).
              We call this number Vapnik-Chevronenkis (VC) dimension</li>
          <li>Factors affecting the training process
            <ul>
              <li>Hyper-parameter value selection: Stratified k-fold cross-validation</li>
              <li>Stopping criteria in iterative optimization: early stopping on hold-out validation set</li>
              <li>Regularisation</li>
              <li>Training set size,</li>
            </ul>
          </li>
          <li>Regularization</li>
            <ul>
              <li>a process to enforce properties in the solution of an optimization problem (e.g. low capacity, smoothness)</li>
              <li>L2 Tikhonov regularisation forces the values of w to be small (leads to smoother decision functions</li>
              <li>L1 regularisation forces w to be sparse, i.e. to have most values equal to zero (leads to simpler and more interpretable solutions)</li>
              <li>used in models for both iterative optimisation and convex optimisation</li>
              <li>Smoothness and semi-supervised learning</li>
              <li>Dropout in NN in iterative optimisation</li>
              <li>Continuous dropout-based training</li>
              <li>Dropout in convex optimization</li>
            </ul>
          <li>Training set size
            <ul>
              <li>When the number of training samples is small (smaller than the number of the model’s parameters), the model tends to overfit</li>
              <li>In neural networks, use data augmentation and transfer learning</li>
              <li>In statistical ML, this problem is addressed using regularization</li>
            </ul>
          </li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/23.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">Lecture Notes</h2>
  
        <h2 class="homework">No homework</h2>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 24: Multilayer Neural Networks, Part 1</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Lots of research about the Multi-Layer Perceptron (MLP)</li>
          <li>Parameters of a neural network
            <ul>
              <li>The number of layers</li>
              <li>The weight matrix</li>
              <li>The activation functions</li>
            </ul>
          </li>
          <li>Three-Layer Neural Networks
            <ul>
              <li>Called Single-hidden Layer Feedforward Neural Network</li>
              <li>This topology is interesting because we can prove that if the number of neurons is the same as the number of samples,
                  then we can approximate everything. This is a proof that shows that all neural networks are universal approximators.</li>
            </ul>
          </li>
          <li>Activation functions
            <ul>
              <li>Any differentiable function can be used as an activation function</li>
              <li>Identity function</li>
              <li>Logistis</li>
              <li>HyperbTan</li>
              <li>ArcTan</li>
              <li>ReLU</li>
              <li>Leaky ReLU leaks some of the information for negative values</li>
              <li>Softmax is commonly used for the neurons of the last layer of the network</li>
            </ul>
          </li>
          <li>The Backpropagation Algorithm
            <ul>
              <li>Training error can be measured using the Mean-Square Error criterion</li>
              <li>Target values cannot be negative for ReLU and Softmax activation functions</li>
              <li>Backpropagation update rule is based on gradient descent</li>
              <li>Stochastic Backpropagation</li>
              <li>Batch Backpropagation</li>
            </ul>
          </li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/24.0.pdf" target="_blank">Slides</a></li>
        </ul>

        <h2 class="lecture-notes">No lecture Notes</h2>
  
        <h2 class="homework">Homework</h2>
        <ul class="homework">
          <li>
            <a href="./hw24.html" target="_blank">Homework 24</a>
            <ul>
              
            </ul>
          </li>
        </ul>
      </div>    
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 25: Multilayer Neural Networks, Part 2</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Improving Backpropagation, learning with momentum</li>
          <li>Radial Basis Function RBF network
            <ul>
              <li>Radial means that a neuron will react the same way based on how nears a sample is to the target value</li>
              <li>Why? approximate a function that has multiple modes or peaks</li>
            </ul>
          </li>
          <li>Auto-Encoder Network
            <ul>
              <li>Bottlenect layer forces the network to find a compressed representations</li>
              <li>Why? To remove noise from the training set and select important features</li>
            </ul>
          </li>
          <li>Self-Organising Map
            <ul>
              <li>Consists of a rectangular grid of $K$ neurons, e.g. $8 \times 8 = 64$ neurons</li>
              <li>The location of each neuron is represented by a weight vector $\mathbf{w}$</li>
              <li>3 step training process; competetion, cooperation and adaptation</li>
              
              <li>Competition: For each sample $\mathbf{x}_i$, find the closest neuron $j^{*}$. Move it closer to that data point</li>
              <li>Cooperation: Determine neighbours of $j^{*}$</li>
              <li>Adaptation: Move the neighbours of $j^{*}$ closer to $j^{*}$, with farther away neighbors moving less</li>
            </ul>
          </li>
          <li>Convolutional Neural Networks
            <ul>
              <li>Take the raw image data as input instead of a vector representation of the image</li>
              <li>Convolutional layer: each neuron applies a filter onto the image and produces a <strong>feature map</strong></li>
              <li>Sub-sampling/pooling layer: resizes the feature maps, average, max and random pooling</li>
              <li>Fully-connected layer: standard vector-based neural network layer</li>
            </ul>
          </li>
        </ul>
  
        <h2 class="slides">Slides (from 56)</h2>
        <ul class="slides">
          <li><a href="slides/24.0.pdf" target="_blank">Slides</a></li>
          <li><a href="extra/algobeans-self-organizing-maps-tutorial.pdf">Algobeans: Self-Organizing Maps Tutorial</a></li>
        </ul>
      </div>
    </div>

    <div class="lecture">
      <button class="collapsible">Lecture 26: Basics of Convolutional Neural Networks</button>
      <div class="content">
        <h2 class="keywords">Keywords</h2>
        <ul>
          <li>Uses of Convolutional Neural Networks</li>
          <li>Classification vs Object Detection</li>
          <li>Classifical pipeline of manual feature extraction</li>
          <li>Training, optimisation problems are highly non-convex</li>
          <li>End-to-End training means that learn the image representation and classifier at the same time</li>
        </ul>
  
        <h2 class="slides">Slides</h2>
        <ul class="slides">
          <li><a href="slides/26.0.pdf" target="_blank">Slides</a></li>
        </ul>
      </div>
    </div>
  </div>
 
  <script src="main.js"></script>
</body>
</html>