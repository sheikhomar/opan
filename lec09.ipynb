{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Unconstrained optimization, Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "from func import Func\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "- **Homework 08:** Got stuck implementing steepest descent algorithm. How to find $\\alpha_k$ using the Secant method? How to choose the two points?\n",
    "- **Newton Search:** Tried to implement Newton's method for minimisation using solve. Doesn't work! Help?!\n",
    "- **Newton Search:** When to stop for minimisation problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Newton's method work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method (sometimes called the Newton-Raphson method) uses first and second derivatives. The idea behind this method is as follows:\n",
    "\n",
    "- Given a starting point $\\mathbf{x}^{(k)}$, we construct a quadratic approximation $q(\\mathbf{x})$ to the objective function $f(\\mathbf{x})$ at that point. We can create such an approximation using the Taylor series expansion of $f$ about the point $\\mathbf{x}^{(k)}$:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}) = \n",
    "    f \\left(  \\mathbf{x}^{(k)}  \\right)\n",
    "  + \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)^T \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "  + \\frac{1}{2} \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)^T F\\left(\\mathbf{x}^{(k)} \\right)  \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "where $F$ is the Hessian.\n",
    "\n",
    "- We then minimize the approximate (quadratic) function $q(\\mathbf{x})$ instead of the original objective function $f(\\mathbf{x})$.  This can be done by apply the FONC to $q$ i.e., taking the derivative of $q(\\mathbf{x})$ and solving it for when $\\nabla q(\\mathbf{x}) = \\mathbf{0}$ where $\\nabla q(\\mathbf{x})$ is given as:\n",
    "\n",
    "$$\n",
    "\\nabla q(\\mathbf{x}) = \n",
    "     \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "   + F\\left(\\mathbf{x}^{(k)} \\right)  \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "- We use the minimizer $\\mathbf{x}^{*}$ of the approximate function as the starting point in the next step i.e., $\\mathbf{x}^{(1)} = \\mathbf{x}^{*}$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - F \\left( \\mathbf{x}^{(k)} \\right)^{-1} \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "- Repeat the procedure iteratively. \n",
    "\n",
    "If the objective function $f$ is quadratic, then the approximation $q$ is exact, and the method yields the true minimizer in one step. \n",
    "\n",
    "If, on the other hand, the objective function is not quadratic, then the approximation will provide only an estimate of the position of the true minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/newtons-method-solve.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Newton's method compare to steepest descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the method of steepest descent uses only first derivatives (gradients) in selecting a suitable search direction. This strategy is not always the most effective. If higher derivatives are used, the resulting iterative algorithm may perform better than the steepest descent method. Newton's method performs better than the steepest descent method **if the initial point is close to the minimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "from func import Func\n",
    "import utils as utils\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "class NewtonSearch:\n",
    "    def __init__(self, f):\n",
    "        self._f = f\n",
    "\n",
    "    def _update_for_minimisation_using_solve(self, x_k):\n",
    "        jacob_at_x_k = self._f.gradient_at(x_k)\n",
    "        hess_at_x_k = self._f.hessian_at(x_k)\n",
    "        delta_x = self._f._x - x_k\n",
    "        \n",
    "        # Setup system of equation\n",
    "        system_equation = hess_at_x_k * delta_x + jacob_at_x_k\n",
    "        \n",
    "        # Solve it the system of equations\n",
    "        result = sy.nonlinsolve(system_equation, list(params))\n",
    "        \n",
    "        # Convert from set to vector\n",
    "        res_vector = sy.Matrix(list(result)[0])\n",
    "        \n",
    "        return x_k - res_vector\n",
    "        \n",
    "    def _update_for_minimisation(self, x_k):\n",
    "        jacob_at_x_k = self._f.gradient_at(x_k)\n",
    "        hess_at_x_k = self._f.hessian_at(x_k)\n",
    "        hess_inv = hess_at_x_k.inv()\n",
    "        return x_k - hess_inv * jacob_at_x_k\n",
    "\n",
    "    def _update_for_root_finding(self, x_k):\n",
    "        raise Exception('Not implemented')\n",
    "\n",
    "    def _run_algorithm(self, starting_point, epsilon, max_iterations, update_rule):\n",
    "        x_k = sy.Matrix(starting_point)\n",
    "        has_converged = False\n",
    "\n",
    "        # Run iterations\n",
    "        for k in range(1, max_iterations):\n",
    "            \n",
    "            # Apply update rule\n",
    "            x_kp1 = update_rule(x_k)\n",
    "            \n",
    "            # Compute increment size: x(k+1) - x(k)\n",
    "            increment_size = (x_kp1 - x_k).norm()\n",
    "            \n",
    "            # Check for convergence\n",
    "            if increment_size < epsilon:\n",
    "                # At this point the increment size from x(k) to x(k+1)\n",
    "                # is small enough so we will stop.\n",
    "                #\n",
    "                # For root finding, we can stop when f(x_k) is close\n",
    "                # to zero e.g. |f(x_k)| < epsilon\n",
    "                has_converged = True\n",
    "\n",
    "            # Update variables\n",
    "            x_k = x_kp1\n",
    "            \n",
    "            print('Iteration {0:2}: step_size={1:.5f}  x({0})={2} '.format(\n",
    "                k, float(increment_size), utils.format_vector(x_k) )\n",
    "            )\n",
    "            if has_converged:\n",
    "                print(' Stopping condition reached!')\n",
    "                break\n",
    "        if not has_converged:\n",
    "            print(' Stopping condition never reached!')\n",
    "\n",
    "        return x_k\n",
    "\n",
    "    def find_minimum(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_minimisation\n",
    "        )\n",
    "\n",
    "    def find_minimum_using_solve(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_minimisation_using_solve\n",
    "        )\n",
    "    \n",
    "    def find_root(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_root_finding\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\left(x_{1} + 10 x_{2}\\right)^{2} + 10 \\left(x_{1} - x_{4}\\right)^{4} + \\left(x_{2} - 2 x_{3}\\right)^{4} + 5 \\left(x_{3} - x_{4}\\right)^{2}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡            2               4              4              2⎤\n",
       "⎣(x₁ + 10⋅x₂)  + 10⋅(x₁ - x₄)  + (x₂ - 2⋅x₃)  + 5⋅(x₃ - x₄) ⎦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, x3, x4 = sy.symbols('x1, x2, x3, x4')\n",
    "f = Func((x1 + 10*x2)**2   + 5*(x3-x4)**2  + (x2 - 2*x3)**4 + 10*(x1 - x4)**4, (x1, x2, x3, x4))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1: step_size=1.63515  x(1)=['1.4127', '-0.8413', '-0.2540', '0.7460'] \n",
      "Iteration  2: step_size=0.54505  x(2)=['0.8836', '-0.7884', '-0.3386', '0.6614'] \n",
      "Iteration  3: step_size=0.18168  x(3)=['0.7072', '-0.7707', '-0.3668', '0.6332'] \n",
      "Iteration  4: step_size=0.06056  x(4)=['0.6484', '-0.7648', '-0.3762', '0.6238'] \n",
      "Iteration  5: step_size=0.02019  x(5)=['0.6288', '-0.7629', '-0.3794', '0.6206'] \n",
      "Iteration  6: step_size=0.00673  x(6)=['0.6223', '-0.7622', '-0.3804', '0.6196'] \n",
      "Iteration  7: step_size=0.00224  x(7)=['0.6201', '-0.7620', '-0.3808', '0.6192'] \n",
      "Iteration  8: step_size=0.00075  x(8)=['0.6194', '-0.7619', '-0.3809', '0.6191'] \n",
      "Iteration  9: step_size=0.00025  x(9)=['0.6192', '-0.7619', '-0.3809', '0.6191'] \n",
      "Iteration 10: step_size=0.00008  x(10)=['0.6191', '-0.7619', '-0.3809', '0.6191'] \n",
      "Iteration 11: step_size=0.00003  x(11)=['0.6191', '-0.7619', '-0.3810', '0.6190'] \n",
      "Iteration 12: step_size=0.00001  x(12)=['0.6191', '-0.7619', '-0.3810', '0.6190'] \n",
      " Stopping condition reached!\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{986969}{1594323}\\\\- \\frac{1214723}{1594323}\\\\- \\frac{607360}{1594323}\\\\\\frac{986963}{1594323}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡  986969 ⎤\n",
       "⎢ ─────── ⎥\n",
       "⎢ 1594323 ⎥\n",
       "⎢         ⎥\n",
       "⎢-1214723 ⎥\n",
       "⎢─────────⎥\n",
       "⎢ 1594323 ⎥\n",
       "⎢         ⎥\n",
       "⎢-607360  ⎥\n",
       "⎢──────── ⎥\n",
       "⎢1594323  ⎥\n",
       "⎢         ⎥\n",
       "⎢  986963 ⎥\n",
       "⎢ ─────── ⎥\n",
       "⎣ 1594323 ⎦"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_search = NewtonSearch(f)\n",
    "newton_search.find_minimum_using_solve((3, -1, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1: step_size=1.82333  x(1)=['1.5873', '-0.1587', '0.2540', '0.2540'] \n",
      "Iteration  2: step_size=0.54505  x(2)=['1.0582', '-0.1058', '0.1693', '0.1693'] \n",
      "Iteration  3: step_size=0.36337  x(3)=['0.7055', '-0.0705', '0.1129', '0.1129'] \n",
      "Iteration  4: step_size=0.24224  x(4)=['0.4703', '-0.0470', '0.0752', '0.0752'] \n",
      "Iteration  5: step_size=0.16150  x(5)=['0.3135', '-0.0314', '0.0502', '0.0502'] \n",
      "Iteration  6: step_size=0.10766  x(6)=['0.2090', '-0.0209', '0.0334', '0.0334'] \n",
      "Iteration  7: step_size=0.07178  x(7)=['0.1394', '-0.0139', '0.0223', '0.0223'] \n",
      "Iteration  8: step_size=0.04785  x(8)=['0.0929', '-0.0093', '0.0149', '0.0149'] \n",
      "Iteration  9: step_size=0.03190  x(9)=['0.0619', '-0.0062', '0.0099', '0.0099'] \n",
      "Iteration 10: step_size=0.02127  x(10)=['0.0413', '-0.0041', '0.0066', '0.0066'] \n",
      "Iteration 11: step_size=0.01418  x(11)=['0.0275', '-0.0028', '0.0044', '0.0044'] \n",
      "Iteration 12: step_size=0.00945  x(12)=['0.0184', '-0.0018', '0.0029', '0.0029'] \n",
      "Iteration 13: step_size=0.00630  x(13)=['0.0122', '-0.0012', '0.0020', '0.0020'] \n",
      "Iteration 14: step_size=0.00420  x(14)=['0.0082', '-0.0008', '0.0013', '0.0013'] \n",
      "Iteration 15: step_size=0.00280  x(15)=['0.0054', '-0.0005', '0.0009', '0.0009'] \n",
      "Iteration 16: step_size=0.00187  x(16)=['0.0036', '-0.0004', '0.0006', '0.0006'] \n",
      "Iteration 17: step_size=0.00124  x(17)=['0.0024', '-0.0002', '0.0004', '0.0004'] \n",
      "Iteration 18: step_size=0.00083  x(18)=['0.0016', '-0.0002', '0.0003', '0.0003'] \n",
      "Iteration 19: step_size=0.00055  x(19)=['0.0011', '-0.0001', '0.0002', '0.0002'] \n",
      "Iteration 20: step_size=0.00037  x(20)=['0.0007', '-0.0001', '0.0001', '0.0001'] \n",
      " Stopping condition never reached!\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{52428800}{73222472421}\\\\- \\frac{5242880}{73222472421}\\\\\\frac{8388608}{73222472421}\\\\\\frac{8388608}{73222472421}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡  52428800 ⎤\n",
       "⎢───────────⎥\n",
       "⎢73222472421⎥\n",
       "⎢           ⎥\n",
       "⎢ -5242880  ⎥\n",
       "⎢───────────⎥\n",
       "⎢73222472421⎥\n",
       "⎢           ⎥\n",
       "⎢  8388608  ⎥\n",
       "⎢───────────⎥\n",
       "⎢73222472421⎥\n",
       "⎢           ⎥\n",
       "⎢  8388608  ⎥\n",
       "⎢───────────⎥\n",
       "⎣73222472421⎦"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_search = NewtonSearch(f)\n",
    "newton_search.find_minimum((3, -1, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
