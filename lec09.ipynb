{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Unconstrained optimization, Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "\n",
    "from func import Func\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "- **Homework 08:** Got stuck implementing steepest descent algorithm. How to find $\\alpha_k$ using the Secant method? How to choose the two points?\n",
    "- **Newton Search in nD (see below):** Tried to implement Newton's method for minimisation using solve. It gives different results than when using the inverse of Hessian. Help!\n",
    "- **Newton Search in nD (see below):** When to break out of the loop for minimisation problems? When are we close to a solution?\n",
    "- **Homework 09:** How to go about solving exercise 9.1 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Newton's method work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method (sometimes called the Newton-Raphson method) uses first and second derivatives. The idea behind this method is as follows:\n",
    "\n",
    "- Given a starting point $\\mathbf{x}^{(k)}$, we construct a quadratic approximation $q(\\mathbf{x})$ to the objective function $f(\\mathbf{x})$ at that point. We can create such an approximation using the Taylor series expansion of $f$ about the point $\\mathbf{x}^{(k)}$:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}) = \n",
    "    f \\left(  \\mathbf{x}^{(k)}  \\right)\n",
    "  + \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)^T \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "  + \\frac{1}{2} \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)^T F\\left(\\mathbf{x}^{(k)} \\right)  \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "where $F$ is the Hessian.\n",
    "\n",
    "- We then minimize the approximate (quadratic) function $q(\\mathbf{x})$ instead of the original objective function $f(\\mathbf{x})$.  This can be done by apply the FONC to $q$ i.e., taking the derivative of $q(\\mathbf{x})$ and solving it for when $\\nabla q(\\mathbf{x}) = \\mathbf{0}$ where $\\nabla q(\\mathbf{x})$ is given as:\n",
    "\n",
    "$$\n",
    "\\nabla q(\\mathbf{x}) = \n",
    "     \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "   + F\\left(\\mathbf{x}^{(k)} \\right)  \\left(\\mathbf{x} - \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "- We use the minimizer $\\mathbf{x}^{*}$ of the approximate function as the starting point in the next step i.e., $\\mathbf{x}^{(1)} = \\mathbf{x}^{*}$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - F \\left( \\mathbf{x}^{(k)} \\right)^{-1} \\nabla f \\left( \\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "- Repeat the procedure iteratively. \n",
    "\n",
    "If the objective function $f$ is quadratic, then the approximation $q$ is exact, and the method yields the true minimizer in one step. \n",
    "\n",
    "If, on the other hand, the objective function is not quadratic, then the approximation will provide only an estimate of the position of the true minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Alternative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/newtons-method-solve.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Root finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/newtons-method-root-finding.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Relationship between root finding and minimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method can be seen as an iterative algorithm to find roots. It is often useful when a system of equations do not have a solution but we want to get close to a solution using a iterative method. For example, we know that the following problem does not have a solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/four-same-sized-circles-problem.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we cannot solve the equations with respect to all the variables i.e., we cannot find values for the variables such that all equations become true simultaneously. Solving in this regard means finding roots of system of equations. This is where Newton's method for root finding comes into the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the same idea about root finding can be applied to find the minimum of a function $f$. Instead of looking for where $f$ intersects with the $x$-axis, we look at its derivative $f'$. Basically, we try to find the root $f'$. The root of $f'$ gives us the minimum of $f$ as illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/newton-root-to-min.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting observation is that lines in the $f'$ corresponds to quadratic in $f$ as illustrated in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-09/newton-approximations.png\" width=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the right side, we see that Newton's method approximates a line. As lines in the $f'$ space corresponds to a quadratic function (a polynomial of degree 2) in the $f$ space, the Newton's method approximates a quadratic. The root of the line at each step in the Newton's method corresponds to the approximation of the minimum of a quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How does Newton's method compare to steepest descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the method of steepest descent uses only first derivatives (gradients) in selecting a suitable search direction. This strategy is not always the most effective. If higher derivatives are used, the resulting iterative algorithm may perform better than the steepest descent method. Newton's method performs better than the steepest descent method **if the initial point is close to the minimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "\n",
    "from func import Func\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "class NewtonSearch:\n",
    "    def __init__(self, f: Func):\n",
    "        self._f = f\n",
    "\n",
    "    def _update_for_minimisation_using_solve(self, x_k):\n",
    "        func = self._f\n",
    "        x = func.get_func_args()\n",
    "        \n",
    "        jacob_at_x_k = func.gradient_at(x_k)\n",
    "        hess_at_x_k = func.hessian_at(x_k)\n",
    "        delta_x = x - x_k\n",
    "        \n",
    "        # Setup system of equation\n",
    "        system_equation = hess_at_x_k * delta_x + jacob_at_x_k\n",
    "        \n",
    "        # Solve it the system of equations\n",
    "        result = sy.nonlinsolve(system_equation, list(x))\n",
    "        \n",
    "        # Convert from set to vector\n",
    "        res_vector = sy.Matrix(list(result)[0])\n",
    "        \n",
    "        return x_k - res_vector\n",
    "        \n",
    "    def _update_for_minimisation(self, x_k: sy.Matrix):\n",
    "        jacob_at_x_k = self._f.gradient_at(x_k)\n",
    "        hess_at_x_k = self._f.hessian_at(x_k)\n",
    "        hess_inv = hess_at_x_k.inv()\n",
    "        return x_k - hess_inv * jacob_at_x_k\n",
    "\n",
    "    def _update_for_root_finding(self, x_k: sy.Matrix):\n",
    "        raise Exception('Not implemented')\n",
    "\n",
    "    def _run_algorithm(self, starting_point, epsilon, max_iterations, update_rule):\n",
    "        x_k = sy.Matrix(starting_point)\n",
    "        has_converged = False\n",
    "\n",
    "        # Run iterations\n",
    "        for k in range(1, max_iterations):\n",
    "            \n",
    "            # Apply update rule\n",
    "            x_kp1 = update_rule(x_k)\n",
    "            \n",
    "            # Compute increment size: x(k+1) - x(k)\n",
    "            increment_size = (x_kp1 - x_k).norm()\n",
    "            \n",
    "            # Check for convergence\n",
    "            if increment_size < epsilon:\n",
    "                # At this point the increment size from x(k) to x(k+1)\n",
    "                # is small enough so we will stop.\n",
    "                #\n",
    "                # For root finding, we can stop when f(x_k) is close\n",
    "                # to zero e.g. |f(x_k)| < epsilon\n",
    "                has_converged = True\n",
    "\n",
    "            # Update variables\n",
    "            x_k = x_kp1\n",
    "            \n",
    "            print('Iteration {0:2}: step_size={1:.5f}  x({0})={2} '.format(\n",
    "                k, float(increment_size), utils.format_vector(x_k) )\n",
    "            )\n",
    "            if has_converged:\n",
    "                print(' Stopping condition reached!')\n",
    "                break\n",
    "        if not has_converged:\n",
    "            print(' Stopping condition never reached!')\n",
    "\n",
    "        return x_k\n",
    "\n",
    "    def find_minimum(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_minimisation\n",
    "        )\n",
    "\n",
    "    def find_minimum_using_solve(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_minimisation_using_solve\n",
    "        )\n",
    "    \n",
    "    def find_root(self, starting_point, epsilon=0.00001, max_iterations=21):\n",
    "        return self._run_algorithm(\n",
    "            starting_point,\n",
    "            epsilon,\n",
    "            max_iterations,\n",
    "            self._update_for_root_finding\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4 = sy.symbols('x1, x2, x3, x4')\n",
    "f = Func((x1 + 10*x2)**2   + 5*(x3-x4)**2  + (x2 - 2*x3)**4 + 10*(x1 - x4)**4, (x1, x2, x3, x4))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_search = NewtonSearch(f)\n",
    "newton_search.find_minimum_using_solve((3, -1, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_search = NewtonSearch(f)\n",
    "newton_search.find_minimum((3, -1, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Nonlinear Least Squares \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Levenberg-Marquardt Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoid(t, A, omega, phi):\n",
    "    return A * np.sin(omega * t + phi)\n",
    "\n",
    "num_points = 20\n",
    "x_start = 0\n",
    "x_end = 10\n",
    "\n",
    "A = 2\n",
    "omega = np.pi/4\n",
    "phi = 0\n",
    "noise = 0.5\n",
    "\n",
    "# Generate data with some noise\n",
    "x = np.linspace(x_start, x_end, num_points)\n",
    "y_pure = sinusoid(x, A, omega, phi)\n",
    "noise_data = np.random.normal(0, noise, y_pure.shape)\n",
    "y = y_pure + noise_data\n",
    "\n",
    "# Data for plotting the sinusoid\n",
    "x_smooth = np.linspace(x_start, x_end, num_points*4)\n",
    "y_smooth = sinusoid(x_smooth, A, omega, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# Use Levenberg-Marquardt algorithm for curve fitting\n",
    "optimal_values, covariance = optimize.curve_fit(sinusoid, x, y)\n",
    "\n",
    "# Generate plot data using the optimal values from the LM algorithm\n",
    "fitted_A, fitted_omega, fitted_phi = optimal_values\n",
    "\n",
    "y_fitted = sinusoid(x_smooth, fitted_A, fitted_omega, fitted_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = utils.prepare_plot(x, y, xlimit=(-.2, 10.2))\n",
    "\n",
    "ax.plot(x_smooth, y_smooth, color='#999999', label='Sinusoid function')\n",
    "ax.scatter(x, y, label='Data points');\n",
    "ax.plot(x_smooth, y_fitted, color='brown', label='Fitted points');\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$f(t)$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
