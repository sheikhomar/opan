{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Lecture 8: Gradient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "from func import Func\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of two decision variables $x_1$ and $x_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/method-of-steepest-descent.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/proposition-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/proposition-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/stopping-criteria.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition $\\triangledown f(x^{(k)}) $, however, is not directly suitable as a practical stopping criterion, because the numerical computation of the gradient will rarely be identically equal to zero. There are several practical stopping criteria:\n",
    "\n",
    "- Stop if the norm $\\lVert \\triangledown f(x^{(k)}) \\rVert$ of the gradient is less than a prespecified threshold\n",
    "- Stop if the absolute difference between objective function values for every two successive iterations is less than some prespecified threshold $\\epsilon$:\n",
    "$$\n",
    "\\left| f(x^{(k+1)}) - f(x^{(k)}) \\right|  < \\epsilon \\hspace{10mm}\n",
    "$$ \n",
    "- Stop if the norm of the difference between two successive iterate is less than a prespecified threshold (if the x's does not change):\n",
    "$$\n",
    "\\left \\lVert x^{(k+1)} - x^{(k)} \\right \\rVert  < \\epsilon \\hspace{10mm}\n",
    "$$\n",
    "- Relative criterion 1 (take the max to avoid division by zero):\n",
    "$$\n",
    "\\frac{\\left| f(x^{(k+1)}) - f(x^{(k)}) \\right|}{ \\max \\left\\{ 1, \\left| f(x^{(k)}) \\right| \\right\\} }\n",
    "< \n",
    "\\epsilon \\hspace{10mm}\n",
    "$$ \n",
    "- Relative criterion 2:\n",
    "$$\n",
    "\\frac{\\left\\lVert x^{(k+1)} - x^{(k)} \\right\\rVert}{ \\max \\left\\{ 1, \\left\\lVert x^{(k)} \\right\\rVert \\right\\} }\n",
    "< \n",
    "\\epsilon \\hspace{10mm}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two (relative) stopping criteria above are preferable to the previous (absolute) criteria because the relative criteria are \"scale-independent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Example 8.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1a.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1b.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.3.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1c.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/quadratic-form.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/quadratic-form-symmetric-q.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/definite-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how can we check the **definiteness** of a quadratic form or equivalently, a symmetric matrix $\\mathbf{Q}$? \n",
    "\n",
    "We can check the eigenvalues of $\\mathbf{Q}$ as stated in **Theorem 3.7**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/definiteness-symmetric-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/characteristic-polynomial.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/theorem-3.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/symmetric-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/theorem-3.3.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/orthogonal-matrices.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Steepest Descent and Quadratic Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/steepest-descent-quadratic-function.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{Q}$ and $\\mathbf{b}$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steepest descent algorithm for the quadratic function can be represented as\n",
    "\\begin{align}\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha_k  \\mathbf{g}^{k}\n",
    "\\end{align}\n",
    "where $\\mathbf{g}^{k} = \\nabla f \\left( \\mathbf{x}^{k}  \\right) = \\mathbf{Qx}-\\mathbf{b}$ and\n",
    "\n",
    "<img src=\"figures/lecture-08/alpha-quadratic.png\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the steepest descent algorithm for quadratic functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/solution-steepest-descent-for-quadratic-functions.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have found an expression for $\\alpha_k$ in the algorithm, which is:\n",
    "$$\n",
    "\\alpha_k = \\frac{ \\mathbf{g}^{(k)T} \\mathbf{g} }{ \\mathbf{g}^{(k)T} \\mathbf{Q} \\mathbf{g}^{(k)} } \\mathbf{g}^{(k)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Convergence of Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma 8.1 and Theorem 8.1 are used to prove Theorem 8.2 which is more important to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/lemma-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that if we have a quadratic function, then the steepest descent algorithm will always converge  to the minimum no matter where we start from. This is an important result because other algorithms like the Newton method, the starting point is very important. We have to start close to the minimum in order to get good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fixed step-size Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing $\\alpha_k$ at each iteration $k$ can be expensive. Sometimes, it may be useful to relax this  condition and use a fixed $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/fixed-step-size-gradient-algorithm.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/fixed-step-size-gradient-algorithm-text.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is whether this simpler algorithm with fixed $\\alpha$ will converge no matter where we start. It turns out that it does if $\\alpha$ is chosen to be between within a certain range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.3.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem 8.3 tells us that the largest range of values of $\\alpha$ for which the gradient algorithm is globally convergent is beween zero and $\\frac{2}{\\lambda_{max}(\\mathbf{Q})}$. This means for $\\alpha$ to converge, we need the value of alpha to be within the range given in Theorem 8.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.4.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that the matrix $Q$ is not symmetric but we can make it symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring constant term: 1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}8 & 2 \\sqrt{2}\\\\2 \\sqrt{2} & 10\\end{matrix}\\right], \\quad \\left[\\begin{matrix}-3\\\\-6\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡ 8    2⋅√2⎤  ⎡-3⎤⎞\n",
       "⎜⎢          ⎥, ⎢  ⎥⎟\n",
       "⎝⎣2⋅√2   10 ⎦  ⎣-6⎦⎠"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = [[4, 2*sy.sqrt(2)],\n",
    "     [0, 5]]\n",
    "b = [[3],\n",
    "     [6]]\n",
    "f_ex84 = Func.create_quadratic(Q, b, 24)\n",
    "f_ex84.as_quadratic_form_for_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "--- \n",
    "## Convergence Rate of Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn our attention to the issue of convergence rates of gradient algorithms. In particular, we focus on the steepest descent algorithm. We first present the following theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.4.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/condition-number-q.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number $r$ is computed using the 2-norm. The Gamma in the MatLAB example is the condition number. If this number is small then we converge quicker. If Gamma is large then the convergance is very slow. The condition number tells us the behaviour the gradient descent algorith.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/convergence-ratio.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.6.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fig86():\n",
    "    x1, x2 = sy.symbols('x1, x2')\n",
    "    f = x1**2  +  x2**2\n",
    "    return Func(f, (x1, x2))\n",
    "fig86 = create_fig86()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}2 & 0\\\\0 & 2\\end{matrix}\\right], \\quad \\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡2  0⎤  ⎡0⎤⎞\n",
       "⎜⎢    ⎥, ⎢ ⎥⎟\n",
       "⎝⎣0  2⎦  ⎣0⎦⎠"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig86.as_quadratic_form_for_gradient_descent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
