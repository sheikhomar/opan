{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Lecture 8: Gradient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "from func import Func\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of two decision variables $x_1$ and $x_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/method-of-steepest-descent.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\nabla f(\\mathbf{x}^{(k)})$ is the gradient (the red arrow) i.e., a vector that has the direction of steepest ascent. The $\\alpha$ determines the magnitude (size) of the gradient. Basically, we can stretch the gradient by picking arbitrary numbers for $\\alpha$. If $\\alpha$ is zero then our gradient will become $\\mathbf{0}$ and when $\\alpha$ is negative then our gradient will point to the opposite direction. We can see that for arbitrary numbers for $\\alpha$ (larger than zero), the term $\\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$ spans a line from $\\mathbf{x}^{(k)}$ as show in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.1-span.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the expression below attempts to find a value for $\\alpha$ that minimises the function $f$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/search-for-alpha.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the point of view of our function $f$, the red line correspond to a plane intersects the surface defined by $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/figure-8.1-span-3d.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below plots the intersection of the parabola and the plane:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/figure-8.1-intersection.png\" width=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the expression below finds the $\\alpha$ value where the function $f$ at $\\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$ is minimum i.e., $f\\left(\\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)}) \\right)$ is minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/search-for-alpha.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the minimum point is also exactly where search line is orthogonal to gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In steepest descent, the largest step step that minimise the objective function $f$ is computed at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Zigzag Behaviour of Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/proposition-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When do we stop the steepest gradient algorithm?\n",
    "\n",
    "The condition $\\triangledown f\\left(\\mathbf{x}^{(k)} \\right) = 0$ is not directly suitable as a practical stopping criterion, because the numerical computation of the gradient will rarely be identically equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/proposition-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/stopping-criteria.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we can stop when difference between two successive iterates is less than a prespecified threshold $\\epsilon$ (which is a small value)\n",
    "$$\n",
    "\\left \\lVert \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)} \\right \\rVert  < \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are several practical stopping criteria:\n",
    "\n",
    "- Stop if the norm $\\lVert \\triangledown f(\\mathbf{x}^{(k)}) \\rVert$ of the gradient is less than a prespecified threshold\n",
    "- Stop if the absolute difference between objective function values for every two successive iterations is less than some prespecified threshold $\\epsilon$:\n",
    "$$\n",
    "\\left| f(\\mathbf{x}^{(k+1)}) - f(\\mathbf{x}^{(k)}) \\right|  < \\epsilon \\hspace{10mm}\n",
    "$$ \n",
    "- Stop if the norm of the difference between two successive iterate is less than a prespecified threshold (if the x's does not change):\n",
    "$$\n",
    "\\left \\lVert \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)} \\right \\rVert  < \\epsilon \\hspace{10mm}\n",
    "$$\n",
    "- Relative criterion 1 (take the max to avoid division by zero):\n",
    "$$\n",
    "\\frac{\\left| f(\\mathbf{x}^{(k+1)}) - f(\\mathbf{x}^{(k)}) \\right|}{ \\max \\left\\{ 1, \\left| f(\\mathbf{x}^{(k)}) \\right| \\right\\} }\n",
    "< \n",
    "\\epsilon \\hspace{10mm}\n",
    "$$ \n",
    "- Relative criterion 2:\n",
    "$$\n",
    "\\frac{\\left\\lVert \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)} \\right\\rVert}{ \\max \\left\\{ 1, \\left\\lVert \\mathbf{x}^{(k)} \\right\\rVert \\right\\} }\n",
    "< \n",
    "\\epsilon \\hspace{10mm}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two (relative) stopping criteria above are preferable to the previous (absolute) criteria because the relative criteria are \"scale-independent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "---\n",
    "## Example 8.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1a.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1b.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.3.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.1c.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      " x(0)=[4, 2, -1]\n",
      " alpha=0.003967294326880356\n",
      " gradient=[0, -2, 1024]\n",
      " x(1)=[4, 2.00793458865376, -5.06250939072548]\n",
      "\n",
      "\n",
      "Iteration 1:\n",
      " x(1)=[4, 2.00793458865376, -5.06250939072548]\n",
      " alpha=0.5000017634507083\n",
      " gradient=[0, -1.98413082269248, -0.00390801102559874]\n",
      " x(2)=[4, 3.00000349891690, -5.06055537832110]\n",
      "\n",
      "\n",
      "Iteration 2:\n",
      " x(2)=[4, 3.00000349891690, -5.06055537832110]\n",
      " alpha=16.197873239052793\n",
      " gradient=[0, 6.99783380930796e-6, -0.00355286043657465]\n",
      " x(3)=[4, 2.99989014889191, -5.00300659533342]\n",
      "\n",
      "\n",
      "Iteration 3:\n",
      " x(3)=[4, 2.99989014889191, -5.00300659533342]\n",
      " alpha=0.5000019583790025\n",
      " gradient=[0, -0.000219702216172735, -4.34855452400123e-7]\n",
      " x(4)=[4, 3.00000000043026, -5.00300637790484]\n",
      "\n",
      "\n",
      "Stopping condition reached. ||g(k)|| = 4.34761968237959E-7\n"
     ]
    }
   ],
   "source": [
    "from steepestd import SteepestDescent\n",
    "x1, x2, x3 = sy.symbols('x1, x2, x3')\n",
    "f = (x1 - 4)**4   +  (x2 - 3)**2  +  4*(x3 + 5)**4\n",
    "f_ex81 = Func(f, (x1, x2, x3))\n",
    "x_0 = sy.Matrix([4, 2, -1])\n",
    "s = SteepestDescent(f_ex81)\n",
    "x_star = s.run(initial_point=x_0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}4\\\\3.00000000043026\\\\-5.00300637790484\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡        4        ⎤\n",
       "⎢                 ⎥\n",
       "⎢3.00000000043026 ⎥\n",
       "⎢                 ⎥\n",
       "⎣-5.00300637790484⎦"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Quadratic Forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/quadratic-form.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/quadratic-form-symmetric-q.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/definite-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how can we check the **definiteness** of a quadratic form or equivalently, a symmetric matrix $\\mathbf{Q}$? \n",
    "\n",
    "We can check the eigenvalues of $\\mathbf{Q}$ as stated in **Theorem 3.7**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/definiteness-symmetric-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/characteristic-polynomial.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/theorem-3.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/symmetric-matrix.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/theorem-3.3.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/orthogonal-matrices.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Steepest Descent and Quadratic Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-08/steepest-descent-quadratic-function.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{Q}$ and $\\mathbf{b}$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steepest descent algorithm for the quadratic function can be represented as\n",
    "\\begin{align}\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha_k  \\mathbf{g}^{k}\n",
    "\\end{align}\n",
    "where $\\mathbf{g}^{k} = \\nabla f \\left( \\mathbf{x}^{k}  \\right) = \\mathbf{Qx}-\\mathbf{b}$ and\n",
    "\n",
    "<img src=\"figures/lecture-08/alpha-quadratic.png\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the steepest descent algorithm for quadratic functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/solution-steepest-descent-for-quadratic-functions.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have found an expression for $\\alpha_k$ in the algorithm, which is:\n",
    "$$\n",
    "\\alpha_k = \\frac{ \\mathbf{g}^{(k)T} \\mathbf{g} }{ \\mathbf{g}^{(k)T} \\mathbf{Q} \\mathbf{g}^{(k)} } \\mathbf{g}^{(k)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Convergence of Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma 8.1 and Theorem 8.1 are used to prove Theorem 8.2 which is more important to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/lemma-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.2.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that if we have a quadratic function, then the steepest descent algorithm will always converge  to the minimum no matter where we start from. This is an important result because other algorithms like the Newton method, the starting point is very important. We have to start close to the minimum in order to get good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fixed step-size Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing $\\alpha_k$ at each iteration $k$ can be expensive. Sometimes, it may be useful to relax this  condition and use a fixed $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/fixed-step-size-gradient-algorithm.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/fixed-step-size-gradient-algorithm-text.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is whether this simpler algorithm with fixed $\\alpha$ will converge no matter where we start. It turns out that it does if $\\alpha$ is chosen to be between within a certain range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.3.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda_{max}(Q)$ is the largest eigenvalue of $Q$.\n",
    "\n",
    "Theorem 8.3 tells us that the largest range of values of $\\alpha$ for which the gradient algorithm is globally convergent is beween zero and $\\frac{2}{\\lambda_{max}(\\mathbf{Q})}$. This means for $\\alpha$ to converge, we need the value of alpha to be within the range given in Theorem 8.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/example-8.4.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that the matrix $Q$ is not symmetric but we can make it symmetric. Here is how we compute it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring constant term: 1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}8 & 2 \\sqrt{2}\\\\2 \\sqrt{2} & 10\\end{matrix}\\right], \\quad \\left[\\begin{matrix}-3\\\\-6\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡ 8    2⋅√2⎤  ⎡-3⎤⎞\n",
       "⎜⎢          ⎥, ⎢  ⎥⎟\n",
       "⎝⎣2⋅√2   10 ⎦  ⎣-6⎦⎠"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = [[4, 2*sy.sqrt(2)],\n",
    "     [0, 5]]\n",
    "b = [[3],\n",
    "     [6]]\n",
    "f_ex84 = Func.create_quadratic(Q, b, 24)\n",
    "Q, b, = f_ex84.as_quadratic_form_for_gradient_descent()\n",
    "Q, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJQAAAAWCAYAAAAmRLUHAAAABHNCSVQICAgIfAhkiAAAA/VJREFUaIHt2luoVFUcx/GPl4oDRmpQhoQFGRVGPXQzKOZYkiSCYr30kEUFgmAigd3MQ1GaUBH1ED3UeciHbpQUYgUlGhSGVnSRKOpUVJaaN+hUJzs9/NfJ4z6zZ2aPozPl/sJmDfP/r71+s2Zd/uu/NyUlR5Eb8D724gAubK+ckjbSgwFsx6s4u+gNzsPf2I9n8AAm5fhegZfxE/5I5Zu4tmijNbgOT2AT9mEQz7Xw/q2kqNaTcStewVfoF5P4XdyC0W3WBxWswvrkv7loo4tSxcV1/O5NfjvwLB7C0/gAq4s2WoOPUjv7sU1nD6iiWhcmnx+xBivFJN6Tvn8Jo9qoL8sWsdiMK9Lo8tTQzBo+1yeft3BiFftxRRqsQzemio6t6OwBVVTrDMwxciWahO9S/flt1JdlTapzetZQaykdk8qBHPtoPIzfRKy1v4pPXt1meAdfih/S6RTV+jZeE7N+ONvxVPpcaYmy4HD7cuh/HZM1HM7efDnOxDrsxmwsw+2YXuA+veKH3XQYWv7PDP15f9Xx69UB/Ti2hm18Kvtz7Ben8mdsxfkZ+0YR/O1oWl3JWNyYPq9vp5AMv6fypKwhb4UaJU5ug/g2x+eUVC5EF64WcdQ0vIEr8WID4u7CueKEU3Ioq0R/rhN9Wouj2Y99qazUc5yFR8QJbdDB/bsaq5PPAVyQsXXh+2Qvsv01SkVnB+XDqWhO6+JUbxsmtljTcCqK65ssBtUAXsCDOIORK9QsLMVF+FTkKvLYncqv8XHG1u/gjLqkgNCSYBEex+fiRPZre+WM4AeR2hgQJ/275QyoJWJfnIuzxKAYEcknvkjlnhz70IDrakbxMcwSPCkmdLc46XUas8XutVVsycdjA9VjqH1YKzLfk8W+XI2N4uQxNd0wy7RU9jWn+ZhkGR4Ticdu/NJeObnMSGUPPjMsPVQrbTAUjOft3zvxvFjR7svYZuIa8fig3unkNJyjyomhBfTqgKN0gywXQfgWXCX6twhHsh+zTEhlX9ZQK20wNOpqDbqluBT3iFPdZkzBPBGs3yZ/SxxiJRbgZjEA8pibLg4+U5w+rM5O3JGpM6S9Xg6n1RTVugD3iz7bpPrjrj61+6fRfmxGX5am+nWFmN2VOn4T8Si+wZ/YJbbMyxpsp1djq0hP8su7+qrU+VBs4ROq2I4kPYpprec/KMUoNejV+GpcVF9eW1MaaOtf7kyV5hWp1EGMFzO+lQ+oS4K1YmycWqTS/FTpdfEQsNWvUBxp5oiMbt4rNyXFOUGkgfamq9CY6MInDl0Kyxfsjl16HDoWVlRzqhWU94sE5wyROhinM3MiJUeHDSII34X3RGqjpKSk5D/EP8yaI9jaTbYtAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$\\left \\{ 6 : 1, \\quad 12 : 1\\right \\}$$"
      ],
      "text/plain": [
       "{6: 1, 12: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues = Q.eigenvals()\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABoAAAASCAYAAABFGc6jAAAABHNCSVQICAgIfAhkiAAAAQ1JREFUOI3t1L8rhVEcx/EX+QuQ/AM2f8RlMSnFzMCglAxKKWVjv7sMbMomNsWoKOku0s1gMvgxGNA1PN+7cJ8fJ90sPnV6Tud8Pud9fjzn8EeaQR1neEELezneASzgELd4wzPOMY/eItBVDP6KRgloMfofsI8t7OAp2g/Qkwcaw0gYaiWgcUx2mPkw7iM7XbSqtspARVqPbL3dULiPv9B7fD+6CerDbNSPuwnaxiiOcFIlUJN+RsuRaaC/aigVtBT+G9mfV1kpoJXwXmMoBZICWgvfJQZTIVVBG+G5UHIm35+IqShk+zyBO9nbB49YjfocdvEpu5jPHcZvhueHNmOGeaWZ4G3hNG+F/6qsL7liU0twp8QzAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$12$$"
      ],
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_max = np.max(list(eigenvalues.keys()))\n",
    "lambda_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "--- \n",
    "## Convergence Rate of Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn our attention to the issue of convergence rates of gradient algorithms. In particular, we focus on the steepest descent algorithm. We first present the following theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/theorem-8.4.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/condition-number-q.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number $r$ is computed using the 2-norm. The Gamma in the MatLAB example is the condition number. If this number is small then we converge quicker. If Gamma is large then the convergance is very slow. The condition number tells us the behaviour the gradient descent algorith.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/convergence-ratio.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-08/figure-8.6.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fig86():\n",
    "    x1, x2 = sy.symbols('x1, x2')\n",
    "    f = x1**2  +  x2**2\n",
    "    return Func(f, (x1, x2))\n",
    "fig86 = create_fig86()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( \\left[\\begin{matrix}2 & 0\\\\0 & 2\\end{matrix}\\right], \\quad \\left[\\begin{matrix}0\\\\0\\end{matrix}\\right]\\right )$$"
      ],
      "text/plain": [
       "⎛⎡2  0⎤  ⎡0⎤⎞\n",
       "⎜⎢    ⎥, ⎢ ⎥⎟\n",
       "⎝⎣0  2⎦  ⎣0⎦⎠"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig86.as_quadratic_form_for_gradient_descent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
