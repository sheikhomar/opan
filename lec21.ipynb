{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21: Generalised Linear Discriminant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.summary {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".insight {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".sidenote {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".warning {\n",
       "  border: solid 1px #8a6d3b !important;\n",
       "  background: #fcf8e3 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".green {\n",
       "  color: #006600 !important;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  color: #cc0000 !important;\n",
       "}</style>Stylesheet \"styles.css\" loaded."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "from fractions import Fraction\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "utils.load_custom_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mimimum Square Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lecture, we looked at linear discriminant functions when the samples are linearly separable. When the samples are linearly separable, we can define an objective function based on misclassified samples $\\chi$. This approach does not yield good results when the two classes are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/minimum-square-error-intro.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What are the target vectors?\n",
    "\n",
    "<img src=\"figures/lecture-21/target-vectors.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to define the objective function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/mse-objective-function.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "<p>Recall in the linearly separable case, the objective function was defined based on misclassified samples.\n",
    "This was done because we assumed that the samples are linearly separable. Well, if the samples are linearly\n",
    "separable then the number of misclassified samples will be zero. If not then the algorithm will not converge.\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "Now, we define a different objective function based on least squares problem. Just by defining the objective\n",
    "function differently, we can approach the problem from a different direction. So whether a classifier\n",
    "works linearly separable data or not is defined on the objective function that we use to the train the \n",
    "classifier.\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "If the samples are linearly separable $\\mathcal{J}_s$ will be zero, otherwise it will be larger than 0.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we use the L2 norm in the objective function? \n",
    "\n",
    "Because we only want a single error value that expresses the entire objective.\n",
    "\n",
    "#### Why do take the square of the norm?\n",
    "- We get an easier expression when we take the derivative. Expression becomes a bit more complicated with square root.\n",
    "- We get a quadratic function which has a single minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to solve the objective function analytically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 4.43 looks like the least-squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/derivative-mse-objective-function.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative to zero we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/mse-derivative-obj-zero.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal $\\mathbf{w}$ can be computed using the above expression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### When is a matrix invertible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In equation 4.45, use the inverse of $\\mathbf{X}\\mathbf{X}^T$. A matrix is invertible or non-singular when it has full rank i.e., when its rank equals the lesser of the number of rows and columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "Suppose $\\mathbf{A}$ is an $N \\times D$ matrix:\n",
    "<ul>\n",
    "<li>The rank is a nonnegative integer that cannot be greater than either $N$ or $D$: $rank(A) \\leq \\min(N, D)$</li>\n",
    "<li>The rank is the number of columns or row of the matrix $\\mathbf{A}$ that are linearly independent.</li>\n",
    "<li>Only the zero matrix has  $N$ or $D$</li>\n",
    "<li>If $\\mathbf{A}$ is a square matrix $N=D$, then $\\mathbf{A}$ is invertible if and only if $\\mathbf{A}$ has rank $N$</li>\n",
    "    <ul>\n",
    "        <li>A square matrix is full rank if and only if its determinant is nonzero.</li>\n",
    "    </ul>\n",
    "<li>If $\\mathbf{A}$ is a square matrix $N=D$ and $\\mathbf{A}\\mathbf{A}^T$ is invertible then $\\mathbf{A}$ is also invertible.</li>\n",
    "<li>For a non-square matrix, it will always be the case that either the rows or columns (whichever is larger in number) are <b>linearly dependent</b></li>\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Full rank in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have samples $\\mathbf{X} \\in \\mathbb{R}^{N\\times D}$. If the number of samples $N$ is higher than the number of dimensions $D$ and the samples are independently and drawn from the same distribution then the matrix $\\mathbf{X}$ can be inverted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $\\mathbf{XX}^T$ is non-singular when the samples $\\mathbf{x}_i, i = 1, 2, \\cdots, N$ are Independent and Identically Distributed (i.i.d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "<strong>Independent and Identically Distributed Assumption:</strong> The i.i.d. assumption has two parts:\n",
    "  <ul>\n",
    "    <li>\n",
    "        Independent: two random variables are not dependent on each other. Think about it this way; if we know the outcome of one random variable, does it give us information as to the outcome of another?\n",
    "    </li>\n",
    "    <li>Identically Distributed: samples come from the same random variable or function. For example, all our samples $\\mathbf{x}_i$ may come from the normal distribution with mean $\\mu_1$ and covariance matrix $\\Sigma_1$ i.e., $\\mathbf{x}_i \\sim N(\\mu_1, \\Sigma_1)$\n",
    "    </li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What is pseudo-inverse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a matrix is not invertible i.e., when the rank of a matrix is not full, then we can use the pseudo-inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\mathbf{X}^{\\dagger} = (\\mathbf{XX}^T)^{-1}\\mathbf{X}$ is called the pseudo-inverse of $\\mathbf{X}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make $\\mathbf{XX}^T$ invertible by using a regularised version of $\\mathbf{X}^{\\dagger}$. This means we change the elements in the diagonal of the matrix $\\mathbf{XX}^T$ by adding a small scaling version of the identity matrix:\n",
    "\n",
    "<img src=\"figures/lecture-21/regularised-version-pseudo-inverse.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon $\\epsilon$ is a very small value. By adding this value, we make the rows and the columns independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "  Note: If $\\mathbf{X}\\mathbf{X}^T$ is invertible then $\\epsilon=0$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our matrix $\\mathbf{X}$ is an $N \\times D$. When we perform the operation $\\mathbf{X}\\mathbf{X}^T$ then the resulting matrix is $N \\times N$. The upperleft submatrix of $D \\times D$ has full rank. For the remaining rows and columns, they can be represented as a linear combination of the rest. Adding a small scaled version of the identity matrix, results in a matrix that is invertible because each row or column become independent of one another. Any column cannot be expressed as a linear combination the remaining columns. The rank for the pseudo-matrix is therefore $D$. See the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-21/why-epsilon.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "    Matlab has a function called <code>pinv(A)</code> which works as follows: \n",
    "    <ul>\n",
    "        <li>Compute the inverse of the matrix A by setting epsilon to zero. If A is invertible, then the inverse can be computed.</li>\n",
    "        <li>If the inverse cannot be computed, it uses a very small value of epsilon like $10^{-15}$</li>\n",
    "        <li>If that does not work, it increments epsilon with another small value.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to solve the objective function iteratively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to optimize the objective function in Eq. 4.43 is by applying an iterative optimization algorithm. \n",
    "Both the analytical approach discussed above and the iterative approach will lead to almost the same $\\mathbf{w}$\n",
    "because the objective function is quadratic which has only one optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/iterative-lms-algorithm.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/iterative-sample-based-lms-algorithm.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Non-Linear Decision Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have used a linear decision function (i.e. a hyperplane in $\\mathbb{R}^D$) separating the feature space in two regions:\n",
    "\n",
    "$$\n",
    "g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + w_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most classification problems cannot be solved linearly. How can we extend what we have learnt until now to define nonlinear decision functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done in two ways:\n",
    "1. Augment the decision function i.e., change the form of the decision function\n",
    "2. Augment the data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to augment the decision function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could take the linear decision function and augment it by adding more terms involving the products of pairs of the elements of $\\mathbf{x}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-21/non-linear-decision-function.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{Q}$ is a symmetric matrix i.e., $Q_{dl} = Q_{ld}$ because $x_d x_l = x_l x_d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\mathbf{Q}$ is a parameter of the classifier, we can define it to be a symmetric and nonsingular matrix. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the decision function shape in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of the classifier are then defined by the properties of the matrix:\n",
    "\n",
    "<img src=\"figures/lecture-21/tilde-weight.png\" width=\"200\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\tilde{\\mathbf{W}}$ is a positive scaled version of the identity matrix, the decision function corresponds to a hypersphere in $\\mathbb{R}^D$\n",
    "\n",
    "<img src=\"figures/lecture-21/hypersphere.png\" width=\"250\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\tilde{\\mathbf{W}}$ is positive definite, the decision function corresponds to a hyperellipsoid in $\\mathbb{R}^D$\n",
    "\n",
    "<img src=\"figures/lecture-21/hyper-ellipsoid-function-3d.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\tilde{\\mathbf{W}}$ is a not positive semi-definite (i.e. some of its eigenvalues are positive and others negative), the decision function corresponds to a hyperboloid in $\\mathbb{R}^D$\n",
    "\n",
    "<img src=\"figures/lecture-21/hyperboloid.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "This is not used in practice. Just to show that it is possible to make a non-linear decision function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### How to augment the data representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way to obtain a nonlinear decision function is to define nonlinear function $\\phi : \\mathbb{R}^D \\to \\mathbb{R}^{M}$ that maps the original data representation $\\mathbf{x}$ in $\\mathbb{R}^D$ to some other feature space $\\mathbb{R}^M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples include:\n",
    "\n",
    "- Example of a mapping from one dimension to two-dimensions: $\\phi(x) = [x, x^2]^T$\n",
    "- Example of a mapping from two dimensions to three-dimensions: $\\phi(x_1, x_2) = [x_1, x_2, x_1 x_2]^T$\n",
    "- Example of a mapping from two dimensions to 4-dimensions: $\\phi(x_1, x_2) = [x_1, x_2, x_1 x_2, \\sin(x_1 x_2)]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a linear decision function using the mapping $\\phi$:\n",
    "$$\n",
    "g(\\mathbf{x}) = \\mathbf{w}^T \\phi(\\mathbf{x})\n",
    "$$\n",
    "corresponds to a nonlinear decision on $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Binary Classifiers in Multi-Class Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a training set $\\mathbf{T} = \\{ (\\mathbf{x_1}, l_1), (\\mathbf{x_2}, l_2), \\cdots,  (\\mathbf{x_N}, l_N) \\}$ where each label $l_i \\in \\{ c_1, c_2, \\cdots, c_K \\}$ can take one of $K$ different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use multiple binary classifiers to solve multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary decision functions can be combined in order to define multi-class classification schemes in two ways:\n",
    "- One-versus-One Classification aka. All-Pairs\n",
    "- One-versus-Rest Classification aka. One-versus-All\n",
    "- Error Correcting Output Codes (ECOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### One-versus-One Classification Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the One-vs-One classification scheme, we use $K(K-1)/2$ classifiers for each pair of classes. For example, in a 3-class classification problem involving classes $c_1$, $c_2$ and $c_3$, we define the following binary decision functions:\n",
    "\n",
    "$$\n",
    "g_{12}(\\mathbf{x}) \\text{ discriminates classes } c_1 \\text{ and } c_2 \\\\\n",
    "g_{13}(\\mathbf{x}) \\text{ discriminates classes } c_1 \\text{ and } c_3 \\\\\n",
    "g_{23}(\\mathbf{x}) \\text{ discriminates classes } c_2 \\text{ and } c_3 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to define the decision functions?\n",
    "\n",
    "To define the decision function $g_{kl}(\\cdot)$, a subset of the original training set is used, which is formed only by the training vectors belonging to classes $c_k$ and $c_l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to perform the classification?\n",
    "\n",
    "Classification is based on **majority voting**. Take the class that got the most votes from all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### What are the Advantages and Disadvantages?\n",
    "\n",
    "- Advantage: The training set for each classifier is smaller, so we get faster training time.\n",
    "- Advantage: More accurate than one-vs-all because we rely on more than one classifier for the classification\n",
    "- Disadvantage: For large $K$, we have trained more binary classifiers, in the order of $O(K^2)$. This becomes expensive during classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### One-versus-Rest Classification Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier for each class so we end up having $K$ different binary classifiers. For example, in a 3-class classification problem involving classes $c_1$, $c_2$ and $c_3$, we define the following binary decision functions:\n",
    "\n",
    "$$\n",
    "g_{1}(\\mathbf{x}) \\text{ discriminates class } c_1 \\text{ from } c_2 \\text{ and } c_3 \\\\\n",
    "g_{2}(\\mathbf{x}) \\text{ discriminates class } c_2 \\text{ from } c_1 \\text{ and } c_3 \\\\\n",
    "g_{3}(\\mathbf{x}) \\text{ discriminates class } c_3 \\text{ from } c_1 \\text{ and } c_2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to define the decision functions?\n",
    "\n",
    "To define the decision function $g_{k}(\\cdot)$, the entire training set is used where the samples belonging to class $c_k$ form the positive class, while the samples belonging to the other classes form the negative class (a new class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to perform the classification?\n",
    "\n",
    "When we want to classify a new sample $\\mathbf{x}_{*}$, that sample is given to all the binary classifiers. \n",
    "- Compute the response of each decision function given the new sample. Each classifier gives a response (e.g. the distance from the decision hyperplane).\n",
    "- Classifiy based on the highest response: The new sample $\\mathbf{x}_{*}$ is assigned the class $c_k$ corresponding to the decision function with the highest response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### What are the Advantages and Disadvantages?\n",
    "\n",
    "- Advantage: Better classification time for large $K$ because we only have $K$ binary classifiers to evaluate.\n",
    "- Disadvantage: If one classifier is wrong (e.g. by giving the highest response), then the classification may be wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Error Correcting Output Codes\n",
    "\n",
    "Error Correcting Output Codes (ECOC) is framework that treats some base learners as noisy channels and uses ECC to correct the prediction errors made by the learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to train the classifiers?\n",
    "\n",
    "For each bit, we train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### How to classify?\n",
    "\n",
    "Pick the closes row of the coding matrix as a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### What are the Advantages and Disadvantages?\n",
    "\n",
    "- Advantage: Much faster if $K$ is large because we only need a logairthmic number of bits compared to the number of classes\n",
    "- Disadvantage: Underlying classifiers may perform poorly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
