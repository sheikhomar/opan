{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.summary {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".insight {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".sidenote {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".warning {\n",
       "  border: solid 1px #8a6d3b !important;\n",
       "  background: #fcf8e3 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".green {\n",
       "  color: #006600 !important;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  color: #cc0000 !important;\n",
       "}\n",
       "\n",
       ".simplex-tableaux {\n",
       "    font-size: 13pt !important;\n",
       "}\n",
       "\n",
       ".last-row td {\n",
       "    border-top: solid 1px Black !important;\n",
       "}\n",
       "\n",
       ".smallest-value {\n",
       "    background-color: Green !important;\n",
       "    color: White !important;\n",
       "    font-weight: bold !important;\n",
       "}</style>Stylesheet \"styles.css\" loaded."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "from fractions import Fraction\n",
    "from func import Func\n",
    "import func\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "utils.load_custom_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 1 a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/sol-ex1a.jpg\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using following procedure:\n",
    "\n",
    "**Step 1:** Convert inequalities to equalites using slack variables\n",
    "\n",
    "**Step 2:** Convert objective function to an equally.\n",
    "\n",
    "**Step 3:** Construct the initial simplex tableaux\n",
    "\n",
    "\n",
    "<img src=\"figures/sol-ex1b.jpg\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step 4:** Find the pivot column by looking at the smallest value at the bottom row. \n",
    "\n",
    "**Step 5:** Find the pivot row by pick the smallest of the ratios: $b_i / a_{i1}$\n",
    "\n",
    "**Step 6:** Make the pivot column to a basis column using elementary row operations.\n",
    "\n",
    "Following row operations are needed:\n",
    "\n",
    "- $R_1  - 2R_3 \\to R_1$\n",
    "- $R_2  - R_3 \\to R_2$\n",
    "- $R_4  + 3 R_3 \\to R_4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the maximum using SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Optimal solution: "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_1 = 20, x_2 = 60, s_1 = 0, s_2 = 0, s_3 = 20, M=180$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import linprog as lp\n",
    "\n",
    "A = [[2, 1],\n",
    "     [1, 1],\n",
    "     [1, 0]]\n",
    "b = [100, 80, 40]\n",
    "c = [3, 2]\n",
    "\n",
    "res = lp.maximise(A, b, c)\n",
    "lp.pretty_print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum for $f(20, 60)=180$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the right hand side of inequality (i) would increase the maximum slightly. This is because of the following reasons.\n",
    "\n",
    "We know that at least one of the extreme points or corners of the feasible area corresponds to an optimal solution according to Theorem 6:\n",
    "\n",
    "<img src=\"figures/exam-jan-2018/theorem-6.png\" width=\"800\" />\n",
    "\n",
    "This corresponds with the result in b). The maximum $(20, 60)$ is the intersection between (i) and (ii). \n",
    "\n",
    "Drawing the level set $f(x_1, x_2)=180$, it becomes clear why the line corresponding to inequality (i) should be moved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $g$ gives the partial derivatives in each dimensions. The partial derivative $\\frac{\\partial g}{\\partial x_1}$ gives us the slope of $g$ in the positive $x_1$ direction, $\\frac{\\partial f}{\\partial x_2}$ gives us the slope of $g$ in the positive $x_2$ direction and $\\frac{\\partial g}{\\partial x_3}$ gives us the slope in the positive $x_3$ direction.\n",
    "\n",
    "Instead of fixing the direction, we can generalise the partial derivatives to compute the slope in any direction that we want. Like the partial derivative, we can define the directional derivative as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{\\mathbf{d}} f(\\mathbf{x}) = \\lim_{h \\to 0} \\frac{g(\\mathbf{x} + h \\mathbf{d}) - g(\\mathbf{x})}{h}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directional derivative:\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{d}} = \\nabla f(\\mathbf{x})^T \\mathbf{d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE0AAAAVBAMAAADx+n4ZAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAEIl2mSJE3e9UMqtmzbsXyEShAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABTklEQVQoFX1RPUvDUBQ9aUxDUmsjBBxEiGSXgt1cHBTB6W2OZnNQJP4CcRQc/AEOpe5SB8dCXJwcMjoYyE/ooJOD3vvyXhr6JHfIPee8cz/eC7AUYokz9U2tY0h+CS8x1JdlxT9IgUNDjZpKj0mXfJtNlXFv3FTWmLCvr7eOj/ekw6UDjYHaZ2ekU9jBzE0mOwInCuOZ5drnTZnyxT+B0k9xqfDkhuXa1y2ZcnwDuTXHucK4AmbFR1Fkcj8/lyZqMqe39FI8EZeYfM1+au5pJ6cH3hLYACq88FE1ViMuo/d5i7dh0e5uUGGeW/Vzjn6GWEkAKx4JFhGuR7DvJcTCJ3mfvmfCiSS5/qUJpYR4/OICeV8WHqjdLcIx4youNODsKGLt05+6231VlJMnGkRDOwAGU83a8yBqP9enLvmGmrRkO0f4305Gyeg9MzRT+AN3jj9ORka7LQAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$e^{x_{2} \\left(x_{1} + 1\\right)}$$"
      ],
      "text/plain": [
       " x₂⋅(x₁ + 1)\n",
       "ℯ           "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,x2 = sy.symbols('x1, x2')\n",
    "g = sy.exp((x1+1)*x2)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the derivative using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAAA0BAMAAADswW6/AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMA74lUIhAydqtEZt3Nu5nxBet/AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEKUlEQVRYCe2YTYgURxTH/8xM93xlXUEUFGH7ENlLEleS4BdiH/RgBG1CxIUky7gXL5odEmTFwzqIH5cF9yAoijgIOWiCmZCICIITCCEJTdhDToGQJcQQFWWjiXrK+F5Vd3VXd3V6dr1aMNPv/fu931R1T3f/Z4D+R8lU2jCJC9CcVO1BYGNKJGGo98gkG7SKmxQ/ug8sSapre39j2bbtydqMfFzTW5xdACwRcBKM97atw4owyd2e1CpmOSMovtVkTvblQ4f/PNwQfR1AxYCCHhE742/50LI3XZ8Zf91FqQMR44QAKOiGOE/E+VAbP9K07A6qsxDx+HIdenERUOAfoGvNo9qibopxHaj4/mqfPoyP6QF66SN/prDnbaDYgU0LFjFBaajln9OJlBmg1uTxs8Oy0BqecncXugXgAxdWBzKOQTdT3eV+oO/is+YtFB7codW5JWf0yvAaWHxyrkPEvHwaPNNdt08Bd0UafzPMdA3WemfwKqZhLcNYQ1SP7XGAT4LGCCqEbiBHGwPUw6+0/2tMuJVVk5/K0tu9DjAg4/3/jXAgjilti23OtGGAAk+p5CZ2zwy2tNpqO566QTIWF2VsgtpPxL6t3qCjN7ylpzI7lBYN0LfL8+DKG6g7QDPdk6ukodXHA/OlNlCYQ7mLMTcXkS5IQ61Th6auUuE1ek1dbadb8pU0NOipOtVGfru5IhN64M0tnrklX82EDvV6+d0ZFZnQjPq+5JfQvg6Tocg1aHQ7p7H4Y0p37sSgR0NxhrXFQ48nkLAfdoA/XghqO3FohZMqQUc5CGbKd5DsUXTT+yqNuCbu3wx9hUsD6IfxinQcW2roUepUFcbBQ4Gh5TbpATTthxR5nUfPUpUpv7KTAOxdpF9RMy22FLQ2p7rCYL0MrGOnCVpohrLyK+RLYn5FQcnGhDOtu2GT2gZQ4AuCWkuVHvoVYSECvzLtf+P7bXGi7C5VyuXTWpJDg+IHtTv0KwdJifxKNNMW6RI6RdHewzs+VqsEdOgbVADNr7yPuF8RULJGqDlUKKG/UIdTu1HvCsPEgAT0c5Y0v1L34n6FoaX7z5pYMkORhH5F58Ir/Ft1xVXBgASUPZPuV8ojokx4QOUz6HvKqoSupBYMzFLO3zXgNd//0ve/51CcKExQkPArXE1D+hWxfE438ZuCAoNNyiWUAv2YMnSwRW/ROB+FtPQgseY4UMsHJhqUZ0DZ3Cb8iunapeuBCgMonaiid5SlDCifqP79ipwpfWMujfyGd0wzPcof/ju9+vcrEkqrG53ce69tgP419BPJ3xG0f78iodFlml4+48iZL2RIaG0u7OGrQow9ocLb6IYSVzNjCcXPQYG4KtLF0a0vvc+gBNCcm3T4mQaASQqg//84qbmm1mxt3wJ+mmdTtD3807z/PxG01uyE/kR4Dg09D74tQEXcAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}x_{2} e^{x_{2} \\left(x_{1} + 1\\right)}\\\\\\left(x_{1} + 1\\right) e^{x_{2} \\left(x_{1} + 1\\right)}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡       x₂⋅(x₁ + 1)   ⎤\n",
       "⎢   x₂⋅ℯ              ⎥\n",
       "⎢                     ⎥\n",
       "⎢          x₂⋅(x₁ + 1)⎥\n",
       "⎣(x₁ + 1)⋅ℯ           ⎦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = sy.Matrix([g]).jacobian((x1, x2)).T\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find the directional derivative in the direction $d=[1,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASQAAAAbBAMAAAAnjO8TAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAq90iEHZEiWbNuzJUme8lQ5C+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEX0lEQVRIDa1XS4gURxj+pqen59Gz7uIDIgHp7CZKLrH1kOQS0iOK5LBmhF00kMXREJTkkAm4BAXjkGjiRZjESxpEGuJJCBkCQRIIbtYHOeSwqMiCevOg4GGMa0JIYP3/qn5Uz5Rj77I/VNf3P+vrv6prdoFB4mqctsa2dFNel9LRGdO2YlolzQ5QbfRZl2Fo9uX8ClyTxtWjulZI35rePPuuA9zotS5Dz/Ut+tt1YMiFNboJtMazxE69So7DLAeoPSt+CfaLqdgR1g4AJoMtgyjlOmpihRWmVHZV8/Lwi6m0gDWihDM0tJSmb+8WZAygOPVzlB1TKrQoMSVvprRe5XJiiMs5qdIBRzClvTR0lAreeqNx8aiLrcBErr0ROY8iEVOqcntVMQNV68PKueFy9mffI++kSot8pvQ7DR0lG98Bge3gI6BTbsH+3KPIhJIVsKrIgJ2s3qMT0o5juVwFP4FLKKVJk12iFbWUgMdA2+xiPwUM08ExPGC9f873W+Is2eES1Tr5WU7LSX1elUrt0wUC04mHypXxLqwRMoWlc77/wKcmcJd+oaHrEuwuXYdVB3RT4BI6MDwCSpe4HklMaUzq6jOkhBJT2pV4uBxm+XpTS7MmKHETdJTeLrbpjnzHxTbku0fQhOFRpKRERFFqsppQ4oPRKylK5UboluXMMzCdVOmA/bM0vqYhKO3Y/fqeOltp36en3NpX04dhruUNM184uMejGSQVGvnrj+oYarCaUCrMUd7kmg+V/UGKUpECgJ2TN2S5mge8DLV0QO63Nq8DbhFgSmaz9IrRxrEfKPSAm5c9GN/eRGGGIkgMjx6ySwzK/CCJNq7SovXwRf1ViK0WvhSlUpts1hjeEC7z5k7gfQGj0kGogQOZUtErLlhu0c21YT7EeEf4Ny869NkJaO+7wzN3Scgf4RxRMmaAwzjufQD+UZCSosS7jV0z5m3hMxb/iouFpQPhoJdsERBdQoVshms/Ru7+5InQzdMhBSMfKuYcg6Lvf/vA90cIrmoAHi6wlb8a4Ijvv+b73zAUx9vuEtpwcq/HFilWK0I8u6EyzrM4SxiuE6VG/m8M8xKJVKPYxAQUvFCLusSUgCf8kJQIpLokKD1ifyKXE5igCYaS0qUO4+oChpsMMkpEiTcO9n+cpadkOeT6h/1ZhClVvX3i1Y06DKJUz5LHMRElPt5XCl3QO+opieO9ifqbqTJT+mTmR/AP5zFKamPczZRIQREl+sat/yvdfEtDqcBX5RD3/j2Y2f7QYkq1yR3zVK40R4lTpwhllIgS7Yu5bmLqFKX1dmlow7/zdGt0yFWY54AMIs+SCDyIjzMkKCERJYzFxl5K0rEt9mcBCSX77J/ns2QkMaYX4tOxbTZC2yPAs3qtq3Y9TiitWlzs6mOea624YYj4UegLV/446fNpDAkljTOryQwGRsaMB0bFzhWhJL7XuGQf+LLPMtCwMpQGLrFU5xaMDvg/bqnVViDeGn3pKT2v5jp/3XDbAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}x_{2} e^{x_{2} \\left(x_{1} + 1\\right)} + \\left(x_{1} + 1\\right) e^{x_{2} \\left(x_{1} + 1\\right)}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡    x₂⋅(x₁ + 1)             x₂⋅(x₁ + 1)⎤\n",
       "⎣x₂⋅ℯ            + (x₁ + 1)⋅ℯ           ⎦"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = D.T * sy.Matrix([1, 1])\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the directional derivative in the point (2,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKsAAAAZBAMAAAC8xMNUAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAq90iMpl272bNiRBUu0QcdQEvAAAACXBIWXMAAA7EAAAOxAGVKw4bAAADHUlEQVRIDe2Uy2sTURSHvzRJk84k6VCsVhAaAy4UwUEpSDeNCzduzML4qlpBrKILX4iItkYEnwvjTlR0KhakWBxdKLoxgoI7iy5c+AoFEVc+CtaY6njuvdPWP6DuHJjMyT2/+8255zG05Fxm+BrNeWRnmKlwUeefYmMF7N7BKli9N8Lo7+3aoq2L/soK1k5Z7hx6A03nYptCmfbAStCWXXybJzo0XJmOtsEhDvOwr7IBUkcEWCAiAjgW/IQr3HWjBZ6XSASBF8q0B+aHmjtYv+kg9WMau8PhGTwgXeAtdwbqgi5hlxX20myX5F6scuMEzVmaBrZhZNoDjfuMhm44xJ4K36aw0TMOw3CXz7orkoK1RrCzCuvInXZI1TP9NJeJqDUjUx5YfsFo2A/t7scqtSms1eTw+TU3eaWVCpv55Vsl9U9tbnbITMhTkqCxRmaw3iT2mk97Xo45nYQTgo0EZyvUVm2vgsLS/uOW/ELbyTU8KpD5DvYiiJzeng9lykPSFay2RLvElyN7k9HaWwXL7Zpv10osDrFWcFlReU1Xtcsj9p3o9W7Jhx/9HcqUh3sIVltyQilu5wJBm761EGxqy5J+O4B1von26eh4VXNJZ7sKxGQLO/TKi1Am78jiKay2QPpJStg/iX2isMdpHPeliuerGpt02HNEbYD4WJgEEnqlxzUy5UnlDTY+JsKilh/yTbS2p7CyY3VloWBLGpvwiakqRcrSjlKy1ETUJT3BXMlWVcu05yGC1ZbEWYBl8L5isKnWYs/B3V8EUZozFe0jQbbJnSgTr6dH5FXNdYU9DD2+lmnP42JxfK224ClRN/CnsGqzwwFJkLvaZ50pWcKFl+KR4Wso63FIjNBQl+FjIVqmPaLYZzTEPBrdPt0OpmSCHeGTz0kinn1UTlOXql4mudWuISN7ypX2uJ9PllTfdpA8YGTGA32hZunQYAun/+5b6/3XUqxVfWoGB/LE59TWw4qN8qmRee8Y6IaHvbPgQ/GdFHljq29kxsNw0Gasa0HwjUxxs1Amo5WzzOT1HyvZ/J8Ek4RcTuZpZq/RnPMH1qsPggnBkuYAAAAASUVORK5CYII=\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}48618.5035654523\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[48618.5035654523]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.evalf(subs={x1:2, x2:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use multivariable Taylor series:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(x_0) + Df(x_0) (x-x_0) +  \\frac{1}{2} (x-x_0)^T D^2 f(x_0) (x-x_0) + \\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFoAAAAbBAMAAAAOgoX4AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAq90iEIl2mUTvVDJmzbuVhH4QAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABg0lEQVQ4EeWUMUvDQBiG36RnDYaqOAiiQ4wW3KyDuJ5DESddHBzcqoJUBOsg6qLg6j8QtC6ORVAntYtODoWuFgL+ARWc9bvv2lzTaPwBHiT3vO/3ENILFIitVKyhovJTqbrF2OAeuNTlgJ+LTq2ODNxuA905OH4WXtQFriNFv0pHgFAwFbeHI3agEtmo02XsrvUb7XlAyEBAEttF2o1dsGoT7tIrUh6gGJakubHPKRi7kqmiF1dwAjouYndZ0tzYJxSMjb4cMliAo34OMWwJq9E4bbzoN7mL2g/qC5ThBlQrtiWBefYhhfDZqbcV+iyiDuFBsy1pru0ywSNd2s6vlcTI8arErATGodkm1vbM5CCwS4FtZwzTagKxkwc2GPm9iYJmQo2A7fkzsc+l/fUBOhhe7tae2gMOQE+VgO2hp6JslrQ5VcMAHQ6vOXVn+10XrftFC9r3ggpsf7bXScx2FkgnSeGM7U2IUtgkAdvpg+ckx8zYNvEP+i+23/l/8uu5OP7oN9KHTjp7EtiWAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}e^{x_{2} \\left(x_{1} + 1\\right)}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡ x₂⋅(x₁ + 1)⎤\n",
       "⎣ℯ           ⎦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,x2 = sy.symbols('x1, x2')\n",
    "g = sy.exp((x1+1)*x2)\n",
    "G = Func(g, (x1, x2))\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor(self, x0):\n",
    "    \"\"\"\n",
    "    Computes the third order Taylor series expansion about the\n",
    "    given point x0.\n",
    "    \"\"\"\n",
    "    # Compute the vector: x-x0 since we are going to use multiple times\n",
    "    x_minus_x0  = self._x - sy.Matrix(x0)\n",
    "\n",
    "    # Compute f(x0)\n",
    "    term1 = self.func_at(x0)\n",
    "\n",
    "    # Compute dfdx(x0)*(x-x0)\n",
    "    term2 = self.gradient_at(x0).T * x_minus_x0\n",
    "\n",
    "    # Compute p1 = 1/2*(x-x0)\n",
    "    p1 = sy.Rational(1, 2) * x_minus_x0\n",
    "\n",
    "    # Compute p2 = p1^T * Hessian(x0)\n",
    "    p2 = p1.T * self.hessian_at(x0)\n",
    "\n",
    "    # Compute term3 = p2 * (x-x0)\n",
    "    term3 = p2 * x_minus_x0\n",
    "\n",
    "    result = term1 + term2 + term3\n",
    "\n",
    "    # Simplify the sum of all the terms\n",
    "    # return sy.simplify(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAAmBAMAAAAW8GwJAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAEO+J3bt2MiKZq81EZlQKtzBKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAFL0lEQVRYCe1XXWhcRRT+bnazf9lst1UKSR+aRN8U3VIMxiLdKgRRMduHKD65VSy2oAaDVu1D90FQQUn8CVKxsBWpQh+6gqi4aBfqH76kqC+CQgQVK1I3KP6guJ6ZOWfvzL03m10JKRHn4c4533zfmW9nzv5hy9gl2Gjj2bFRbNloprXfUVz83/CdinoZM1HgBcVC510I2XkQ+DEEXmgg6DtTCTp65HkgHUKDrPXOg75nwwbeB7xqGF4ThO7y342g71fDZcg33g7DNpJs2Fn3sbpLJLrn+0z2Pf7CQfPeKwLtmFnK972+Iip6NArsBlO1s9VumAGO8Z3In4w3Zq+qIFWEjnHc56naP/hpVPR6FNgNpmrj2m6YAY7xncS7QClZRLYEHc+O+jxV+wM/jYjSZQKD1xTBC0PadzwfXlgNkf7+Hah5TXNnFGMP3d80jb2Aqn24Y5052jh0TUbhHZ02DRhdQPtOzAcWhxZuCyChlH0nm0kgV0SyBOiYfMtQtT+RJHKeIDR0TYY5jNhfkRoDat+ppsvwqtjdcCEry+lWML7nYrUYsK0CrwgTW76/IdFnIvxOAnu+Qid8TbzAxGeAMZsaiLVv3MJorqyDdB59Iwz5ExccvmZZYcb38JfjO+Adp3wPdKz6hMeTUyeAlySL8p3SlQLXxMRLgVMVEYdn43uBF9h3XxEZdQjukJ0HLd+aMbS5ANzHZN+3BmpSQ9SSq3lwhB7Ba2Li6TxOzShS9FB3CdzJi+x7YBmZ3xjyJ9k55HuqVQT6DfGhP5Z8BXX+WclY7e1/7cNxwdLqVQWvSbYBpvIOW1Rq1ncJLOYNyL4piS9j68Gn7iobXD+lYMi3Xs22HVoSDLUTVl+Ez8s3IbZA7UuvtdRe9q9JtkH2ZzjsNtcPjsyY2Pe9WPAKg5PxGiZu59cEKRjtG9/75fxoXztk9Q7szn+Mq3FSLQxUZdm6JtkG8SoctnCtea5hEt/3jYjlY8vZSqySqTFRCtq+W6sPpd5Vr19Xr79DUR7T9LwBRyo0xefp0a5AsUUEjgbY2+pqvKVom5SI5u0NesTq9Tcuq9erFCI9D09fY7yS1O/QXe2dYftW3O6GvGr8SvyboU9qQPkODiGaLxWLHSRSHjrvA4q0qUxH0kjJx78U7OSbG1epA0PUSfOG/0l1X3+VHjQclRDPIVWBzTZc82SFvjVCpE9y83gCMF2f0x97tCgFO/nmxrW34JjVXyeaUE0/qeBMySw6KiYmC0hUHLbhOorg5wn9Cbgnl7+ffjvQiZdZ0I1vblx7C0ed/bu/mTpLJzyv4MSIWXRUvM1jOydecdmG6yg+ZYzPO3X9zheLi0t34GnCJ4QvvhPB7x0h0MyNayESGrV3Yt+Brwj6QsNZvkpHxducbrV+cdlSSc2sCHxfDtC7tTi8f+sxOplBfTKKywXTL/95jDL5PahW7KEb1wY4llet02whO6OCy3kRlsohOmwh61kr3mRI+ttinMFHnLkFV/I9aWntcLOdHD7/bV7lzwloqRyiXhe2kPWsFB7fFzxdzV5Pvnd+L+duwRV8m8a1K0TFV+pPX+ABXuysErZdSSsGqzbkxH2tVtMBJFnBt2lcIa02Z5YMozeV0mhFXDebKdHtM9q3NG6XVbyaJvaoIo1RtH/cd7mdokX7jmzFDlXP6bVeVfT3T71BUqUOlVdaivYd1YorVVB4rqGevapYQX9veh/Rvnuvs96K/32v74lv3PPePn3r+h7VGux29/QykoceXoNK61vi8UMz/wA6KYtKePOEKgAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{x_{2} \\left(x_{1} - 1\\right)}{2} + x_{2} \\left(\\frac{x_{1}}{2} + 2 x_{2} - \\frac{1}{2}\\right) + 2 x_{2} + 1\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡x₂⋅(x₁ - 1)      ⎛x₁          1⎞           ⎤\n",
       "⎢─────────── + x₂⋅⎜── + 2⋅x₂ - ─⎟ + 2⋅x₂ + 1⎥\n",
       "⎣     2           ⎝2           2⎠           ⎦"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = taylor(G, (1, 0))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAAAaBAMAAAAUFZBoAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAq90iEHZEiWbNuzJUme8lQ5C+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAACeklEQVRIDa2UP2gUQRTGP+/2dkMu6JEQxOpk5QQbs1iIjTgBg+0JCVaScCIWFh6oiMXhkcom+KdbSEgKq4CYVkEioo1EE8VChDR2NubQqIXCOv9nZ2832Q0u3O6bN+/7fjM7tw8oeHmfPxZTDPtBIcF1HMpf3/LbOJy/nFcewWqBFZVrhQFHcWYm/5ryApzN5RXtuk50uGuQFzAO75c2a+ho5+AdAXYDDDWFB/3jfFNu3oaMXquM/ZRp52Cd5Ad8AerqaOeU384A4F4BwCJBXR6C23ZllB/gTI1cvinWNXH+5IWmWiKgXhHNjBFR9nTtDV0ZuxQgoVHp2A7OYbZ5jGuc9kCjdJ+H/GYA7m+Isq0okvPSKalJAdzAHXIJeAZ4xNt2A3QeE2FiAKUaZJm01ztIalIABMtU9fwD4GBwiWKCfWwXXhg+/BqGNe64CYgyPgJuh+HxMHxAR5bGpO1D/slkLXY7QE+gFFR/sDh2BpUNNmRlptnppdoanY79i6p/mJwDVlcoYKYsPyv9it6iHICXmWannWyNThvAq0oPkxwwRKZRIXTl24xodlBtoxKIMtPspFNS0w9w/w72yl0OuPbiCU5R4xJ9UexSOzhxqzMsy0yzk05JjQZME/klO6OTF+9SN/qKxqcmPnVp2KE/dinAYhR9V2VYJ3xOfQdJjQK831ro2r2InwHXDvAjjQGEo7w35FM5iaHWWGmr2RnAFVwVKodIs9hDN7uzsSSgNVbaArxUgur82iMV9z/n+lM0k6GJA06PjUrp/ijqpbqwpGl2VkmGJg6w6rMHptll15iZPQBMszM22dEeANlmaTMU4PtB2sz/ybX82j+fl7SoAoW/tAAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}x_{1} x_{2} + 2 x_{2}^{2} + x_{2} + 1\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡            2         ⎤\n",
       "⎣x₁⋅x₂ + 2⋅x₂  + x₂ + 1⎦"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.simplify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is an optimisation problem with equality constraints, we can use the Lagrangian method.\n",
    "\n",
    "The Lagrangian method is given as follows:\n",
    "1. Define the Lagrangian function by incoporating the constraint into the objective function\n",
    "$$\n",
    "L(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda ( h(\\mathbf{x}) - c  )\n",
    "$$\n",
    "where $\\lambda$ is the Lagrangian multiplier associated with the contraint. When the contraint holds i.e., when $h(\\mathbf{x})=c$ then $L(\\mathbf{x}, \\lambda)=f(\\mathbf{x})$\n",
    "2. Since $L(\\mathbf{x}, \\lambda)$ is an unconstraint objective function, we can optimise it by finding its FONC i.e., taking the derivative and setting it zero.\n",
    "3. Now, we get a system of linear equations. Solving it will yield the points where the objective function is maximised or minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIoAAAAcBAMAAABMljoJAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAq90iEHZEiWbNuzJUme8lQ5C+AAAACXBIWXMAAA7EAAAOxAGVKw4bAAACQUlEQVQ4EaWVv2sUQRTHv9nb29272/xASSvniYiVm0bSCBshiE04IcFKPBSxsDCClsGgjY1w2C1aXJEqEJJaEIOmtDis8gekSXdgFEQhvjezb2bnbpWYe8XMmzfvfe67s/vmgPFsbSsdD8DVYTLRHZ/iJY1vTDnTSjTsVNq8TuVH0LqEpmacWlt8BMwZimjLoSeevLZDIW0nLi0krpFvtQCszbV5dzm0+szrqE9DkcLaHPN7znJ4McGv5SEeuRTW5liN08otPgD8LtB492XDoShtXBOLptflBIouPOGnfwZMHR8PHIrSxnWGcoFX5RYxZUn27LlobRwXSqUpWaOzotQ6+YaiLN66eruttXFYKNU+PfvK2QckfMQUJaQEZUzxV6OLXtcmCqW+DtzAi/Zl4L3d1Z6iRFLElDANj4LE5gnF2wWe4nl6Hx++2t0CJW7mYaUF9R4tdTeGWfbmMMtmKDDVAVJskod7POBmxvaWXaWlMWCXTJ0LptvFm0K0MAX4zoOmsJdbGeXjDmC7USj8RGj84rpyStDkPTLWEqd3UU35ptAxofDp7lUHWP4bxTndx7vb4L6TbhQKvcjgd31QIdiIlip/dZOrNLCxloWVxf118qUbhUKC/dnlO69oa5gyee7nPlCjg1CmT1f70o1Cge2AYYouuK4neUdqFfXzoJ/mju3GT3nEncxHXdBiutGk1pPcvXZl1gStwzeDNkux3Sh78HvGLXPMjxTuOtuNtuLfN+ZLkziHlvwfmdj/OkHr/B/4+X2ItFsLPwAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$\\left[\\begin{matrix}x_{2}^{2} + \\left(x_{1} - 1\\right)^{2}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡  2           2⎤\n",
       "⎣x₂  + (x₁ - 1) ⎦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = sy.symbols('x1, x2')\n",
    "f = (x1 - 1)**2 + x2**2\n",
    "h = Fraction(1,9)*x1**2 + Fraction(1,4)*x2**2 - 1\n",
    "F = Func(f, (x1, x2), constraints=[h])\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lagrangian(self):\n",
    "    num_constraints = len(self._constraints)\n",
    "\n",
    "    # Define a lambda symbol for each constraint\n",
    "    lambda_names = tuple([('lambda%s' % i) for i in range(1, num_constraints+1)])\n",
    "    lambdas = sy.symbols(lambda_names)\n",
    "    lambda_vec = sy.Matrix(lambdas)\n",
    "\n",
    "    # Define constraints as vector\n",
    "    H = sy.Matrix(self._constraints)\n",
    "\n",
    "    # Formulate the Lagrangian function l\n",
    "    l = self._f - lambda_vec.T * H\n",
    "\n",
    "    # Find the gradient of the Lagrangian function\n",
    "    # with respect to all parameters\n",
    "    all_params = tuple(self._x) + lambdas\n",
    "    Dl = l.jacobian(all_params)\n",
    "\n",
    "    # Solve it with respect to all the parameters\n",
    "    result = sy.nonlinsolve(Dl, all_params)\n",
    "\n",
    "    # Remove the lambda value from the results\n",
    "    points = [tup[0:-num_constraints] for tup in list(result)]\n",
    "    lambdas = [tup[-num_constraints:] for tup in list(result)]\n",
    "\n",
    "    return points, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, lambdas = solve_lagrangian(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$$\\left [ \\left ( -3, \\quad 0\\right ), \\quad \\left ( \\frac{9}{5}, \\quad - \\frac{8}{5}\\right ), \\quad \\left ( \\frac{9}{5}, \\quad \\frac{8}{5}\\right ), \\quad \\left ( 3, \\quad 0\\right )\\right ]$$"
      ],
      "text/plain": [
       "[(-3, 0), (9/5, -8/5), (9/5, 8/5), (3, 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting f(-3, 0) = 16.0000000000000\n",
      "Evaluting f(9/5, -8/5) = 3.20000000000000\n",
      "Evaluting f(9/5, 8/5) = 3.20000000000000\n",
      "Evaluting f(3, 0) = 4.00000000000000\n"
     ]
    }
   ],
   "source": [
    "F.evalf(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum is $(-3, 0)$ and the minimum point are $(9/5, -8/5)$ and $(9/5, 8/5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stop when we reach maximum number of iterations e.g. 1000, or stop when the current position does not change much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is the mechanism for which the algorithm attempts to escape local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input to the objective function is discrete then we have a discrete optimisation otherwise we have a continuous optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A candidate solution is path that can be represented as an array of locations e.g. [A, B, C, D, E, F]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we represent the problem as a graph, the neighbour algorithm could find the set of locations that have edges to a given location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is more likely to accept worse candidate solution when the temperature is hot i.e. at the beginning of the algorithm. As the iteration index $k$ increases, the algorithm becomes increasingly reluctant to choose to a worse point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 e. vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.array([[1, 1],\n",
    "               [2, 0]])\n",
    "C2 = np.array([[ 0, -2],\n",
    "               [-1, -1]])\n",
    "X_test = np.array([[ 2,  0, -1, -2],\n",
    "                   [ 0,  0,  1,  0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Compute the class centroid for each class\n",
    "mu1 = np.mean(C1, axis=1).reshape(-1, 1)\n",
    "mu2 = np.mean(C2, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Compute distances between the centroids and the test samples\n",
    "pair_dist_mu1 = pairwise_distances(X_test.T, mu1.T)\n",
    "pair_dist_mu2 = pairwise_distances(X_test.T, mu2.T)\n",
    "pair_dist = np.concatenate([pair_dist_mu1, pair_dist_mu2], axis=1)\n",
    "\n",
    "# Classify using the lowest distance\n",
    "np.argmin(pair_dist, axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_5, x_6$ and $x_7$ is classified to class 1 and $x_8$ is classified to class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([C1, C2], axis=1)\n",
    "l = [1, 1, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAAUBAMAAABPB9NaAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAds0yiUTdVO8Qq2aZuyL3HddZAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA/klEQVQ4EWMQUmagFHCrBTC4UmYI55m7BgwsEGMqJmCahU2MvRND3SwGlq8QYziXrMcwBpsYwyyvnxjGnGRg6IC5Zj+GMQwM2MS4MY1pY2BY/wDqKWxasIlhMUZ/AsN6A8qNAfoyfwI1jGH9QoWwYWBgW0AVY84wUMMYxgNUMaaGgY8KEc7swMAIM0Z+AgPnJ7QEik2MEZj85iugKHzm4rMMGjan+nU2MKShpmRsYjwrPp1mYMtEMUb///9vsCAGSbCjGgNWi02MoRLFGDAHmvxAbBZMWaxiDA6YCpGMmYgpy4BNjFMAUyGSMRswZRmwiXFjUQc0RozyQpRRLQAAWMtcebaZV/QAAAAASUVORK5CYII=\n",
      "text/latex": [
       "$$\\left [ 1, \\quad 1, \\quad 1, \\quad 2\\right ]$$"
      ],
      "text/plain": [
       "[1, 1, 1, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications = []\n",
    "for i in range(X_test.shape[1]):\n",
    "    x_i = X_test[:,i].reshape(-1, 1)\n",
    "    # Compute the distance each sample in the training set\n",
    "    pair_dist = pairwise_distances(X.T, x_i.T).reshape(-1)\n",
    "    \n",
    "    # Assign the label of the closest sample\n",
    "    index = np.argmin(pair_dist, axis=0)\n",
    "    label = l[index]\n",
    "    \n",
    "    classifications.append(label)\n",
    "classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_5, x_6$ and $x_7$ is classified to class 1 and $x_8$ is classified to class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal projection vector $\\mathbf{w}$ can be computed as follows:\n",
    "\n",
    "<img src=\"figures/lecture-19/fisher-ratio-solution-2b.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "The within-class scatter matrix $\\mathbf{S}_w$ can be computed as follows:\n",
    "\n",
    "<img src=\"figures/lecture-19/within-scatter-matrix-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.mean(C1, axis=1).reshape(-1, 1)\n",
    "m2 = np.mean(C2, axis=1).reshape(-1, 1)\n",
    "\n",
    "S1 = np.dot((C1 - m1), (C1 - m1).T)\n",
    "S2 = np.dot((C2 - m2), (C2 - m2).T)\n",
    "Sw = S1 + S2\n",
    "Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.dot(np.linalg.inv(Sw), m1-m2)\n",
    "w = w.reshape(-1)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection vector is $\\mathbf{w}=[1, 1]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/sol-ex-4d.png\" width=\"400\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Baye's decision rule to classify the test samples:\n",
    "\n",
    "- Classify $x$ as $c_1$ if $P(c_1 \\mid x) > P(c_2 \\mid x)$; otherwise classify $x$ as $c_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes' formula, we have:\n",
    "\n",
    "$$\n",
    "P(c_k \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid c_k) P(c_k)}{p(\\mathbf{x})}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K p(\\mathbf{x} \\mid c_k) P(c_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p(\\mathbf{x})$ is the same for when computing $P(c_k\\mid \\mathbf{x})$, it is not necessary to compute it when we want to use the conditional probability to classify new samples. Therefore, the decision rule becomes:\n",
    "\n",
    "- Classify $x$ as $c_1$ if $p(x \\mid c_1)P(c_1) \\geq p(x \\mid c_2) P (c_2)$; otherwise classify $x$ as $c_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probabilities $P(c_1)$ and $P(c_2)$ are given:\n",
    "- $P(c_1) = 500/1500=1/3$\n",
    "- $P(c_2) = 1000/1500=2/3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_c1 = 1/3\n",
    "P_c2 = 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x_c1 = np.array([0.2, 0.1, 0.05, 0])\n",
    "p_x_c2 = np.array([0, 0.05, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1/15, 1/30, 1/60, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_c1_x = p_x_c1 * P_c1\n",
    "P_c1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1/20, 1/10, 1/5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_c2_x = p_x_c2 * P_c2\n",
    "P_c2_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1/15, 1/30, 1/60, 0],\n",
       "       [0, 1/20, 1/10, 1/5]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a matrix so we can easily compute argmin\n",
    "P_ck_x = np.concatenate([[P_c1_x], \n",
    "                         [P_c2_x]])\n",
    "P_ck_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification(P, objective='max'):\n",
    "    labels = np.argmin(P, axis=0)+1 if objective=='min' else np.argmax(P, axis=0)+1\n",
    "    out = 'Classification Result: \\n$$\\n'\n",
    "    for i, label in enumerate(labels):\n",
    "        out += 'x_{} \\\\to c_{}  \\\\\\\\ \\n'.format(i+1, label)\n",
    "    #['x{} => c{}'.format(i+1, label) for i, label in enumerate(labels)]\n",
    "    out += '$$'\n",
    "    return HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Classification Result: \n",
       "$$\n",
       "x_1 \\to c_1  \\\\ \n",
       "x_2 \\to c_2  \\\\ \n",
       "x_3 \\to c_2  \\\\ \n",
       "x_4 \\to c_2  \\\\ \n",
       "$$"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_classification(P_ck_x, objective='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = np.array([[  0, 0.4],\n",
    "                   [0.3,   0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1/50, 1/25, 2/25]), array([1/50, 1/100, 1/200, 0])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(P_ck_x)\n",
    "n_actions = Lambda.shape[1]\n",
    "R_ai_x = []\n",
    "for i in range(n_actions):\n",
    "    lambda_i = Lambda[i,:]\n",
    "    R_ai_x.append(0.0)\n",
    "    for k in range(n_classes):\n",
    "        R_ai_x[i] += lambda_i[k] * P_ck_x[k]\n",
    "R_ai_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Classification Result: \n",
       "$$\n",
       "x_1 \\to c_1  \\\\ \n",
       "x_2 \\to c_2  \\\\ \n",
       "x_3 \\to c_2  \\\\ \n",
       "x_4 \\to c_2  \\\\ \n",
       "$$"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_classification(R_ai_x, objective='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because the samples in each class in the projected space would be mixed together and therefore difficult to discriminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the test samples using a linear classifier, we can augment the data representations using a mapping that transforms the original samples to another feature space where the samples become linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the second order polynomial kernel function:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = \n",
    "\\begin{bmatrix}\n",
    "x_1^2 \\\\\n",
    "\\sqrt{2} x_1 x_2 \\\\\n",
    "x_2^2 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision function in the original feature space:\n",
    "\n",
    "<img src=\"figures/sol-ex-6a.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision function in the transformed space would be a 2D plane that looks something like the grey plane in the figure below:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/exam-jun-2018/sol-ex4.2.b.png\" width=\"200\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 6 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/sol-ex-6c.jpg\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use a linear activation function in the hidden layer. To simplify the proof, we ignore the bias terms. Suppose we have a three-layer network (one hidden layer and one output neuron). The output neuron is computed:\n",
    "\n",
    "$$\n",
    "o_{ML} = \\mathbf{w}^{(2)T} \\mathbf{h}_1\n",
    "$$\n",
    "\n",
    "The $\\mathbf{h}_1$ is computed as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1 = \\mathbf{W}^{(1)T} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Combining the expressions, we get:\n",
    "\n",
    "$$\n",
    "o_{3L} = \\mathbf{w}^{(2)T} \\mathbf{W}^{(1)T} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "The expression above can be simplied:\n",
    "$$\n",
    "o_{3L} = \\mathbf{w}^T_{c} \\mathbf{x}\n",
    "$$\n",
    "where $\\mathbf{w}_{c} = \\mathbf{w}^{(2)T} \\mathbf{W}^{(1)T}$\n",
    "\n",
    "This expression is linear discriminant function which means that the three-layer network will not be able to handle samples that are not linearly separable when the activation functions for all neurons are linear.\n",
    "\n",
    "A neural network that only uses linear activation functions cannot model a nonlinear function even though the network consists of large number of hidden layers and neurons. Therefore, non-linear activation functions are important in the neural network for this task because the samples are not linearly separable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
