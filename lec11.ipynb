{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: Global Search Methods, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative algorithms in previous chapters, in particular gradient methods,\n",
    "Newton's method, conjugate gradient methods, and quasi-Newton methods,\n",
    "start with an initial point and then generate a sequence of iterates. Typically,\n",
    "the best we can hope for is that the sequence converges to a local minimizer.\n",
    "For this reason, it is often desirable for the initial point to be close to a global\n",
    "minimizer. Moreover, these methods require first derivatives (and also second\n",
    "derivatives in the case of Newton's method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we discuss various search methods that attempt to search throughout the entire feasible set:\n",
    "\n",
    "- They use only objective function values and do not require derivatives. Consequently, they are applicable to a much wider class of optimization problems.\n",
    "\n",
    "- In some cases, they can be used to generate \"good\" initial (starting) points for the iterative methods discussed previously.\n",
    "\n",
    "- Some of the methods (specifically, the **randomized search methods**) are also used in combinatorial optimization, where the\n",
    "feasible set is finite (discrete), but typically large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Randomised Search Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A randomized search method, also sometimes called a **probabilistic search method**,  is an algorithm that searches the feasible set $\\Omega$ of an optimization problem by considering randomized samples of candidate points in the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation problem that we want to solve has the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-11/constrained-min-problem.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any $\\mathbf{x} \\in \\Omega$, we assume there exist a \n",
    "set $N(\\mathbf{x}) \\subset \\Omega$ (subset) from which we can generate\n",
    "a random sample. \n",
    "\n",
    "Typically, $N(\\mathbf{x})$ is a set of points that are \"close\" to $\\mathbf{x}$, \n",
    "and for this reason we usually think of $N(\\mathbf{x})$ as a \"neighborhood\"\n",
    "of $\\mathbf{x}$. We use the term **neighborhood** for $N(\\mathbf{x})$ even \n",
    "in the general case where the points in it are arbitrary, not necessarily \n",
    "close to $\\mathbf{x}$.\n",
    "\n",
    "When we speak of generating a random point in $N(\\mathbf{x})$, we mean that\n",
    "there is a prespecified distribution over $N(\\mathbf{x})$, and we sample a \n",
    "point with this distribution. Often, this distribution is chosen to be uniform \n",
    "over $N(\\mathbf{x})$; other distributions are also used, including Gaussian\n",
    "and Cauchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Naive Random Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive random search algorithm whould like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set $k := 0$. Select an initial point $\\mathbf{x}^{(0)} \\in \\Omega$. Typically, the initial point is selected randomly.\n",
    "\n",
    "2. Select a next-candidate point $\\mathbf{z}^{(k)}$ at random from $N\\left(\\mathbf{x}^{(k)}\\right)$. \n",
    "    \n",
    "3. If $f\\left(\\mathbf{z}^{(k)}\\right) < f\\left(\\mathbf{x}^{(k)}\\right)$ then \n",
    "   set $\\mathbf{x}^{(k+1)} := \\mathbf{z}^{(k)}$; otherwise $\\mathbf{x}^{(k+1)} := \\mathbf{x}^{(k)}$\n",
    "   \n",
    "4. If stopping criterion is satisfied, then stop.\n",
    "\n",
    "5. Set $k := k + 1$, go to step 2.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has the familiar form:\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{d}^{(k)}$ is randomly generated. By design, the direction $\\mathbf{d}^{(k)}$  is either $\\mathbf{0}$ or a descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Issues with Naive Random Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with the naive random search method is that it may get stuck in a region around a local minimizer.\n",
    "\n",
    "Here is an example where the algorithm may get stuck in a local minimizer. Suppose that our initial point $\\mathbf{x}^{(0)}$ is a local minimizer. Imagine also that the \"neighbourhood\" of $\\mathbf{x}^{(0)}$ i.e., $N\\left(\\mathbf{x}^{(0)}\\right)$ is sufficiently small that all points in it have no smaller objective function value than $\\mathbf{x}^{(0)}$. In such case, the the algorithm will be stuck in that local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we prevent the algorithm getting stuck in a region around a local minimizer?\n",
    "\n",
    "- One way is to ensure that at each $k$, the neighborhood $N\\left(\\mathbf{x}^{(k)}\\right)$ is a very large set. \n",
    "\n",
    "    In the extreme case, we can set $N\\left(\\mathbf{x}^{(k)}\\right)$ to be the feasible set $\\Omega$. In this case, running $k$ iterations of the naive random search algorithm amounts to finding the best point among $k$ randomly chosen points in $\\Omega$.\n",
    "    \n",
    "    The problem is when the neighborhood is too large the search algorithm\n",
    "    results in a slow search process, because the sampling of candidate points to\n",
    "    consider is spread out, making it more unlikely to find a better candidate\n",
    "    point.\n",
    "\n",
    "- Another approach is to choose with some probability a candidate point $\\mathbf{z}^{(k)}$ even though it does not result in a lower function value than $\\mathbf{x}^{(k)}$ i.e., the candidate point $\\mathbf{z}^{(k)}$ is worse than $\\mathbf{x}^{(k)}$. The **simulated annealing** algorithm incorporates such a mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing is an instance of a **randomized search method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated annealing works like the Naive Random Search method. The difference is that it may (with some probability) choose a worse candidate point $\\mathbf{z}^{(k)}$ as $\\mathbf{x}^{(k+1)}$. Initially, the\n",
    "algorithm jumps around and is more likely to climb out of regions around local minimizers, but with time it settles down and is more likely to spend time around a global minimizer.\n",
    "\n",
    "In other words, as the iteration index $k$ increases, the algorithm becomes increasingly reluctant to choose to a worse point. The intuitive reason for this behavior is that initially we wish to actively explore the feasible set, but with time we would like to be less active in exploration so that we spend more time in a region around a global minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set $k := 0$. Select an initial point $\\mathbf{x}^{(0)} \\in \\Omega$. Typically, the initial point is selected randomly.\n",
    "\n",
    "2. Select a next-candidate point $\\mathbf{z}^{(k)}$ at random from $N\\left(\\mathbf{x}^{(k)}\\right)$. \n",
    "    \n",
    "3. If $p\\left(  k, f\\left(\\mathbf{z}^{(k)}\\right), f\\left(\\mathbf{x}^{(k)}\\right)   \\right) = 1$  then \n",
    "   set $\\mathbf{x}^{(k+1)} := \\mathbf{z}^{(k)}$; otherwise $\\mathbf{x}^{(k+1)} := \\mathbf{x}^{(k)}$\n",
    "   \n",
    "4. If stopping criterion is satisfied, then stop.\n",
    "\n",
    "5. Set $k := k + 1$, go to step 2.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Acceptance Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major difference between simulated annealing and naive random search\n",
    "is that in **step 3**, there is some probability that we set the next iterate to be\n",
    "equal to the random point selected from the neighborhood, even if that point\n",
    "turns out to be worse than the current iterate. This probability is called \n",
    "**acceptance probability**. A typical choice is:\n",
    "\n",
    "$$\n",
    "p\\left(  k, f\\left(\\mathbf{z}^{(k)}\\right), f\\left(\\mathbf{x}^{(k)}\\right)   \\right) = \n",
    "\\min \\left\\{ 1, \\exp\\left(  - \\frac{   \n",
    "f(\\mathbf{z}^{(k)}) - f(\\mathbf{x}^{(k)})\n",
    "}{T_k}     \\right)   \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $T_k$ represents a positive sequence called the **temperature schedule** or **cooling schedule**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acceptance probability $p$ works as follows:\n",
    "\n",
    "- If $f\\left(\\mathbf{z}^{(k)}\\right) \\leq f\\left(\\mathbf{x}^{(k)}\\right)$, then $p$ returns 1, which means that we set $\\mathbf{x}^{(k+1)}$ to $\\mathbf{z}^{(k)}$.\n",
    "\n",
    "- If $f\\left(\\mathbf{z}^{(k)}\\right) > f\\left(\\mathbf{x}^{(k)}\\right)$, then there is still a probability of setting $\\mathbf{x}^{(k+1)}$ to $\\mathbf{z}^{(k)}$. This probability is equal to:\n",
    "\n",
    "$$\n",
    "\\exp\n",
    "  \\left(\n",
    "    - \n",
    "    \\frac{\n",
    "      f(\\mathbf{z}^{(k)}) - f(\\mathbf{x}^{(k)})\n",
    "     }{\n",
    "       T_k\n",
    "     }\n",
    "  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Temperature Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is typical to let the \"temperature\" $T_k$ be monotonically decreasing to 0 (hence the word cooling). In other words, as the iteration index $k$ increases, the algorithm becomes increasingly reluctant to move to a worse point. An appropriate cooling schedule is\n",
    "\n",
    "$$\n",
    "T_k = \n",
    "\\frac{\n",
    "  \\gamma\n",
    "}{\n",
    "  \\log(k + 2)\n",
    "}\n",
    "$$\n",
    "where $\\gamma > 0$ is a problem-dependent constant. It must large enough to allow the algorithm to \"climb out\" of regions around local minimizers that are not global minimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Best Point So Far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has the familiar form:\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{d}^{(k)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{d}^{(k)}$ is randomly generated. The direction $\\mathbf{d}^{(k)}$ may be:\n",
    "\n",
    "- $\\mathbf{0}$ \n",
    "- a descent direction\n",
    "- an ascent direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the direction of simulated annealing algorithm may be an ascent direction, it is good idea to keep track of the best point that we have seen so far; $\\mathbf{x}^{(k)}_{best}$. At each iteration $k$, this point is updated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-11/best-so-far-point-update.png\" width=\"600\" />\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
