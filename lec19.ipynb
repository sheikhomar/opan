{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.summary {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".insight {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".sidenote {\n",
       "  border: solid 1px green !important;\n",
       "  background: #cdffd8 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".warning {\n",
       "  border: solid 1px #8a6d3b !important;\n",
       "  background: #fcf8e3 !important;\n",
       "  padding: 10px !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       ".green {\n",
       "  color: #006600 !important;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  color: #cc0000 !important;\n",
       "}</style>Stylesheet \"styles.css\" loaded."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy\n",
    "import utils as utils\n",
    "from fractions import Fraction\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sympy print pretty math expressions\n",
    "sy.init_printing()\n",
    "\n",
    "utils.load_custom_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Prototype Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of $N$ samples, each represented by a vector $\\mathbf{x}_i \\in \\mathbb{R}^D$, and the corresponding labels $l_i$, we can define three classifiers that can be used in order to classify a new (unknown) sample $\\mathbf{x}_{*} \\in \\mathbb{R}^D$:\n",
    "- Nearest Class Centroid\n",
    "- Nearest Sub-class Centroid \n",
    "- $k$-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these methods classify new samples based on nearest mean vector corresponding to a class, they are also known as **Nearest Prototype Classifiers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Nearest Class Centroid (NCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neasrest Class Centroid represents each class $c_k$ with the corresponding mean class vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/ncc-class-representation.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the class mean vectors $\\mu_1, \\mu_2, \\cdots, \\mu_K$, the new sample $\\mathbf{x}_{*}$ is classified to the class $c_k$ corresponding to the\n",
    "smallest distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/ncc-classification-of-new-sample.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates the mean vectors $\\mu_k$ as red points and the red lines represents the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/ncc-classifier-figure.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "The NCC classifier assumes that each class is unimodal and follows a Normal Distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Nearest Sub-Class Centroid (NCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSC assumes that each subclass $m$ of class $c_k$ follows a Normal Density (distribution) and is represented by the mean subclass vector $\\mu_{km}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/nsc-class-representation.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "- $q_i$ denotes the subclass label of vector $\\mathbf{x}_i$\n",
    "- $N_{km}$ to denote the number of samples forming subclass $m$ of class $c_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the class mean vectors, the new sample $\\mathbf{x}_{*}$ is classified to the class $c_k$ corresponding to the smallest distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/nsc-classification-new-sample.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define the subclasses of each class, we usually apply a clustering algorithm (e.g. K-Means) using the samples of the corresponding class. The number of subclasses per class is a parameter of the NSC and needs to be decided beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows an example where $m=2$ (two subclass per class). The red dots represents the mean vectors and the red line represents the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/nsc-2-subclass-figure.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Nearest Neighbour (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the extreme case where the number of subclasses per class is equal to the number of its samples, then the resulting classifier is called Nearest Neighbor\n",
    "(NN) classifier. That is, NN classifier classifies $\\mathbf{x}_{*}$ to the class of the sample closest to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use multile $k$ nearest neighbors for classification. Given $k=3$, we can compute the mean of the any three samples in our dataset and classify a new sample $\\mathbf{x}_{*}$ based on the class of the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/kNN.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "    Notice that $k$ is an odd number because kNN is based on a voting mechanism where it classifies based on a majority vote. If $k$ is even then there may be cases where the votes are equal. If there are no majority then the classification is arbitrary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below illustrates what happens to the decision boundary when $k$ is an even number. Play with kNN at http://vision.stanford.edu/teaching/cs231n-demos/knn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/lecture-19/knn-different-ks.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of dimensions of the feature space in which the data representations live in is high (comparable, or even higher than the number of samples), the\n",
    "application of statistical techniques is problematic. This is, roughly, because the\n",
    "number of parameters is higher than the number of observations, and thus, their\n",
    "estimation is infeasible. This problem is usually called as curse of dimensionality. For this reason, dimensionality reduction techniques, i.e. techniques that can\n",
    "find an optimal (with respect to an associated criterion) mapping from the original\n",
    "feature space $\\mathbb{R}^D$ to a low-dimensional feature space $\\mathbb{R}^d$ where $d < D$ are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fisher Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/figure-pca-vs-lda.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/linear-projection-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (4.5) can be thought as follows (illustrated in the figure below):\n",
    "1. The vector $\\mathbf{w}$ is the normal to the line that we are projecting the vectors $\\mathbf{x}_i$ onto. Notice how different vectors of $\\mathbf{w}$ corresponds to different lines.\n",
    "2. Computing the dot product between the vector $\\mathbf{w}$ to any sample $\\mathbf{x}_i$, would result in a value $y_i$ that lies in a line represented by its normal $\\mathbf{w}$. Notice that the magnitude of the vector $\\mathbf{w}$ does not effect the corresponding line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/eq-4.5-explain.png\" width=\"300\" /> <img src=\"figures/lecture-19/eq-4.5-figure-2.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figures above, we assume that each class is unimodal and follows a Normal Distribution. This is the assumention of Fisher Discriminant Analysis. The goal with Fisher Discriminant Analysis and Linear Discriminant Analysis is to project our samples into a lower dimensional space (in our case a line) such that discrimination between the classes are highest. In other words, we want to find a line represented by $\\mathbf{w}$ so:\n",
    "\n",
    "1. The distance between mean values of each class in the projected space is as large as possible, and\n",
    "2. The variance within each class in the projected space is small as possible\n",
    "\n",
    "The figure below shows an example where both criteria are optimised;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/lda-goal.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Objectives\n",
    "We know that if we assume that samples in each class $c_k$ are unimodal and follow a normal distribution, then they are better discriminated when:\n",
    "\n",
    "1. The distance between mean values of each class in the projected space is as large as possible, and\n",
    "2. The variance within each class in the projected space is small as possible\n",
    "\n",
    "We have to come up with an optimisation function expressed in terms of our vector $\\mathbf{w}$ and our samples $\\mathbf{x}_i$ that maximises objective (1) while minimisng objective (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The first objective can be expressed as a function of $\\mathbf{w}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/difference-mean-values.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m_k$ is the projected mean value of class $c_k$ given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/mean-value-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\mathbf{S}_b$ is a $D\\times D$ matrix called the **between-class scatter matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/eq.4.15-between-class-scatter.png\" width=\"600\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)**  The second objective can be expressed as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for computing the variance in the projected space within each class is given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/variance-within-projected-class-samples.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to minimise the variance of both classes, we just compute the sum of the variances and come up with a function in terms of $\\mathbf{w}$ and $\\mathbf{x}_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/sum-of-variances.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{S}_w$ is $D\\times D$ matrix called **with-class scatter matrix**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/within-scatter-matrix-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for scatter matrix is given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/scatter-matrix-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fisher Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an expression for both our objectives, we formulate an optimisation problem. Maximising the following optimisation problem called Fisher's Ratio, finds the optimal projection vector $\\mathbf{w}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/fishers-ratio.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solving Fisher's Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of 4.16 is given by solving the generalized eigenanalysis problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/fisher-ratio-solution-1.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that $\\mathbf{S}_w$ is non-singular i.e., $\\mathbf{S}_w$ has an inverse then we can rewrite (4.17) to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/fisher-ratio-solution-2a.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the optimal $\\mathbf{w}$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/fisher-ratio-solution-2b.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the two-class case, the rank of $\\mathbf{S}_b$ is equal to one because it is calculated as an outer product of one vector. This restricts the number of possible solutions to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis extends Fisher Linear Discriminant in problems involving more than two classes. In order to define the optimisation problem for LDA we need to redefine the two scatter matrices to account for more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Redefine Objective 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The between-class scatter matrix $\\mathbf{S}_b$ becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/lda-between-class-scatter-matrix.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a matrix with rank equal to $K - 1$, restricting the number of possible\n",
    "solutions to $K - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Redefine Objective 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The within-class scatter matrix $\\mathbf{S}_w$ becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_w = \\sum_{k=1}^{K} \\mathbf{S}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{S}_k$ is the scatter matrix for class $c_k$ given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/scatter-matrix-formula.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimisation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need $K-1$ of the $\\mathbf{w}$ vectors to classify $K$ classes. Recall that in Fisher Discriminant Analysis where $K=2$, we only needed one $\\mathbf{w}$ vector i.e., $K-1$. We organise all these vectors into an $D\\times K-1$ matrix $\\mathbf{W}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can formulate an optimisation problem with respect to $\\mathbf{W}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/lda-optimisation-function.png\" width=\"600\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $Tr(\\dot)$ denotes the trace operator of a matrix. The trace operator of a matrix sums the diagonal entries for a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"sidenote\">\n",
    "We usually add the contraint $\\mathbf{W}^T \\mathbf{W} = \\mathbf{I}$ because we want the $\\mathbf{w}$ vectors to be orthonormal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solving LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of Eq. 4.22 is given by sequentially applying generalized eigenanalysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lecture-19/fisher-ratio-solution-1.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and keeping the eigenvectors corresponding to the maximal $K - 1$ eigenvalues in\n",
    "order to form the columns of $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have obtained the $\\mathbf{W}$, we can classify a new sample $\\mathbf{x}_{*} \\in \\mathbb{R}^d$ by the projection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y}_{*} = \\mathbf{W}^T \\mathbf{x}_{*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
